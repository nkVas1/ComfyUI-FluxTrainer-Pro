# -*- coding: utf-8 -*-
"""
Low VRAM Training Utilities for Flux.2 Models
==============================================

–≠—Ç–æ—Ç –º–æ–¥—É–ª—å –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª—è–µ—Ç –∞–≥—Ä–µ—Å—Å–∏–≤–Ω—ã–µ —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –ø–∞–º—è—Ç–∏
–¥–ª—è –æ–±—É—á–µ–Ω–∏—è LoRA –Ω–∞ Flux.2 –º–æ–¥–µ–ª—è—Ö —Å –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–Ω—ã–º VRAM (8GB –∏ –º–µ–Ω–µ–µ).

–û—Å–Ω–æ–≤–Ω—ã–µ —Ç–µ—Ö–Ω–∏–∫–∏:
1. Sequential Block Processing - –æ–±—Ä–∞–±–æ—Ç–∫–∞ –±–ª–æ–∫–æ–≤ –ø–æ –æ–¥–Ω–æ–º—É
2. CPU Offloading - –≤—ã–≥—Ä—É–∑–∫–∞ –≤–µ—Å–æ–≤ –º–æ–¥–µ–ª–∏ –Ω–∞ CPU/RAM
3. Gradient Checkpointing - –ø–µ—Ä–µ—Å—á–µ—Ç –∞–∫—Ç–∏–≤–∞—Ü–∏–π –≤–º–µ—Å—Ç–æ —Ö—Ä–∞–Ω–µ–Ω–∏—è
4. Mixed Precision Training - FP8/BF16 —Å–º–µ—à–∞–Ω–Ω–∞—è —Ç–æ—á–Ω–æ—Å—Ç—å
5. Optimizer State Offloading - —Å–æ—Å—Ç–æ—è–Ω–∏–µ –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä–∞ –≤ RAM

Author: ComfyUI-FluxTrainer Team
License: Apache-2.0
"""

import gc
import time
import threading
from concurrent.futures import ThreadPoolExecutor
from dataclasses import dataclass, field
from enum import Enum
from typing import Any, Callable, Dict, List, Optional, Tuple, Union

import torch
import torch.nn as nn
from torch.cuda.amp import autocast

from .device_utils import clean_memory_on_device

import logging
logger = logging.getLogger(__name__)


class OffloadStrategy(Enum):
    """–°—Ç—Ä–∞—Ç–µ–≥–∏–∏ –æ—Ñ—Ñ–ª–æ–∞–¥–∏–Ω–≥–∞ –¥–ª—è —Ä–∞–∑–Ω—ã—Ö —Å—Ü–µ–Ω–∞—Ä–∏–µ–≤."""
    NONE = "none"                    # –ë–µ–∑ –æ—Ñ—Ñ–ª–æ–∞–¥–∏–Ω–≥–∞ (–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ VRAM)
    CONSERVATIVE = "conservative"    # –¢–æ–ª—å–∫–æ Text Encoders –Ω–∞ CPU
    AGGRESSIVE = "aggressive"        # –ú–æ–¥–µ–ª—å –∏ –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä –Ω–∞ CPU, –±–ª–æ–∫–∏ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ
    EXTREME = "extreme"              # –í—Å—ë –Ω–∞ CPU, –æ–¥–∏–Ω –±–ª–æ–∫ –∑–∞ —Ä–∞–∑, FP8


@dataclass
class LowVRAMConfig:
    """–ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –¥–ª—è –æ–±—É—á–µ–Ω–∏—è —Å –Ω–∏–∑–∫–∏–º VRAM."""
    
    # –û—Å–Ω–æ–≤–Ω—ã–µ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏
    strategy: OffloadStrategy = OffloadStrategy.AGGRESSIVE
    available_vram_gb: float = 8.0
    available_ram_gb: float = 32.0
    
    # Block swapping
    blocks_to_swap: int = 20  # –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –±–ª–æ–∫–æ–≤ –¥–ª—è —Å–≤–∞–ø–∏–Ω–≥–∞ CPU<->GPU
    swap_async: bool = True   # –ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω—ã–π —Å–≤–∞–ø –¥–ª—è –ø–µ—Ä–µ–∫—Ä—ã—Ç–∏—è –≤—ã—á–∏—Å–ª–µ–Ω–∏–π
    
    # Gradient checkpointing
    gradient_checkpointing: bool = True
    cpu_offload_checkpointing: bool = True  # –ê–∫—Ç–∏–≤–∞—Ü–∏–∏ –Ω–∞ CPU
    
    # Precision
    use_fp8_base: bool = True         # FP8 –¥–ª—è –±–∞–∑–æ–≤–æ–π –º–æ–¥–µ–ª–∏
    use_bf16_training: bool = True    # BF16 –¥–ª—è –æ–±—É—á–µ–Ω–∏—è
    
    # Text Encoder offloading
    cache_text_encoder_outputs: bool = True
    text_encoder_offload_to_cpu: bool = True
    
    # Optimizer offloading
    optimizer_offload_to_cpu: bool = True
    optimizer_pin_memory: bool = True  # Pinned memory –¥–ª—è –±—ã—Å—Ç—Ä–æ–≥–æ —Ç—Ä–∞–Ω—Å—Ñ–µ—Ä–∞
    
    # VAE settings
    vae_slicing: bool = True           # Slice VAE –¥–ª—è —ç–∫–æ–Ω–æ–º–∏–∏ –ø–∞–º—è—Ç–∏
    vae_tiling: bool = True            # Tiled VAE
    vae_offload: bool = True           # VAE –Ω–∞ CPU
    
    # Batch settings (–¥–ª—è low VRAM –æ–±—ã—á–Ω–æ 1)
    effective_batch_size: int = 1
    gradient_accumulation_steps: int = 4  # –ù–∞–∫–æ–ø–ª–µ–Ω–∏–µ –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤
    
    # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏
    empty_cache_frequently: bool = True
    use_channels_last: bool = True    # Channels last memory format
    compile_model: bool = False       # torch.compile (—ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞–ª—å–Ω–æ)
    
    def estimate_memory_usage(self, model_params_billions: float) -> Dict[str, float]:
        """–û—Ü–µ–Ω–∫–∞ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –ø–∞–º—è—Ç–∏ –≤ GB."""
        # –ü—Ä–∏–º–µ—Ä–Ω—ã–µ —Ä–∞—Å—á–µ—Ç—ã
        base_model_fp8 = model_params_billions * 1.0  # 1 byte per param
        base_model_fp16 = model_params_billions * 2.0  # 2 bytes per param
        
        lora_rank_64_fp16 = 0.1  # ~100MB –¥–ª—è rank 64
        
        # –ê–∫—Ç–∏–≤–∞—Ü–∏–∏ –∑–∞–≤–∏—Å—è—Ç –æ—Ç —Ä–∞–∑–º–µ—Ä–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –∏ batch size
        activations_per_block = 0.5  # ~500MB per block peak
        
        optimizer_adam_fp32 = model_params_billions * 8.0  # 8 bytes (2 states)
        optimizer_adam_offloaded = 0.0  # –ù–∞ CPU
        
        return {
            "base_model_vram": base_model_fp8 if self.use_fp8_base else base_model_fp16,
            "lora_weights_vram": lora_rank_64_fp16,
            "activations_peak": activations_per_block * (1 if self.gradient_checkpointing else 20),
            "optimizer_vram": 0.0 if self.optimizer_offload_to_cpu else optimizer_adam_fp32,
            "optimizer_ram": optimizer_adam_fp32 if self.optimizer_offload_to_cpu else 0.0,
        }


class CPUOffloadOptimizer:
    """
    –í—Ä–∞–ø–ø–µ—Ä –¥–ª—è –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä–∞, —Ö—Ä–∞–Ω—è—â–∏–π —Å–æ—Å—Ç–æ—è–Ω–∏—è –Ω–∞ CPU.
    –ü–µ—Ä–µ–Ω–æ—Å–∏—Ç –≥—Ä–∞–¥–∏–µ–Ω—Ç—ã –Ω–∞ CPU, –æ–±–Ω–æ–≤–ª—è–µ—Ç –≤–µ—Å–∞ —Ç–∞–º, –∏ –ø–µ—Ä–µ–Ω–æ—Å–∏—Ç –æ–±—Ä–∞—Ç–Ω–æ –Ω–∞ GPU.
    """
    
    def __init__(
        self,
        optimizer: torch.optim.Optimizer,
        device: torch.device,
        pin_memory: bool = True,
        async_transfer: bool = True
    ):
        self.optimizer = optimizer
        self.device = device
        self.pin_memory = pin_memory
        self.async_transfer = async_transfer
        
        # –ü–µ—Ä–µ–Ω–æ—Å —Å–æ—Å—Ç–æ—è–Ω–∏–π –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä–∞ –Ω–∞ CPU
        self._move_optimizer_to_cpu()
        
        # –î–ª—è –∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ–≥–æ –ø–µ—Ä–µ–Ω–æ—Å–∞
        self.stream = torch.cuda.Stream() if async_transfer and device.type == "cuda" else None
        
        logger.info(f"CPUOffloadOptimizer initialized with pin_memory={pin_memory}, async={async_transfer}")
    
    def _move_optimizer_to_cpu(self):
        """–ü–µ—Ä–µ–Ω–æ—Å–∏—Ç —Å–æ—Å—Ç–æ—è–Ω–∏—è –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä–∞ –Ω–∞ CPU."""
        for group in self.optimizer.param_groups:
            for p in group["params"]:
                if p in self.optimizer.state:
                    state = self.optimizer.state[p]
                    for key, value in state.items():
                        if isinstance(value, torch.Tensor):
                            if self.pin_memory and value.device.type == "cuda":
                                state[key] = value.cpu().pin_memory()
                            else:
                                state[key] = value.cpu()
    
    def step(self, closure: Optional[Callable] = None):
        """–í—ã–ø–æ–ª–Ω—è–µ—Ç —à–∞–≥ –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä–∞ —Å CPU offloading."""
        
        # –°–æ–±–∏—Ä–∞–µ–º –≥—Ä–∞–¥–∏–µ–Ω—Ç—ã –Ω–∞ CPU
        for group in self.optimizer.param_groups:
            for p in group["params"]:
                if p.grad is not None:
                    # –ü–µ—Ä–µ–Ω–æ—Å–∏–º –≥—Ä–∞–¥–∏–µ–Ω—Ç –Ω–∞ CPU
                    p.grad_cpu = p.grad.cpu()
                    if self.pin_memory:
                        p.grad_cpu = p.grad_cpu.pin_memory()
        
        # –û—Å–≤–æ–±–æ–∂–¥–∞–µ–º GPU –ø–∞–º—è—Ç—å –æ—Ç –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤
        for group in self.optimizer.param_groups:
            for p in group["params"]:
                if p.grad is not None:
                    p.grad = None
        
        if self.device.type == "cuda":
            torch.cuda.empty_cache()
        
        # –û–±–Ω–æ–≤–ª—è–µ–º –≤–µ—Å–∞ –Ω–∞ CPU
        # (–∑–¥–µ—Å—å –Ω—É–∂–Ω–∞ —Å–ø–µ—Ü–∏–∞–ª—å–Ω–∞—è –ª–æ–≥–∏–∫–∞ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä–∞)
        # –ü–æ–∫–∞ –ø—Ä–æ—Å—Ç–æ –≤—ã–∑—ã–≤–∞–µ–º —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–π step
        
        # –í–æ—Å—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º –≥—Ä–∞–¥–∏–µ–Ω—Ç—ã –∏ –¥–µ–ª–∞–µ–º —à–∞–≥
        for group in self.optimizer.param_groups:
            for p in group["params"]:
                if hasattr(p, "grad_cpu"):
                    p.grad = p.grad_cpu.to(self.device)
                    del p.grad_cpu
        
        self.optimizer.step(closure)
        
        # –û—á–∏—â–∞–µ–º –≥—Ä–∞–¥–∏–µ–Ω—Ç—ã
        self.optimizer.zero_grad(set_to_none=True)
    
    def zero_grad(self, set_to_none: bool = True):
        """–û—á–∏—â–∞–µ—Ç –≥—Ä–∞–¥–∏–µ–Ω—Ç—ã."""
        self.optimizer.zero_grad(set_to_none=set_to_none)
    
    @property
    def param_groups(self):
        return self.optimizer.param_groups
    
    def state_dict(self):
        return self.optimizer.state_dict()
    
    def load_state_dict(self, state_dict):
        self.optimizer.load_state_dict(state_dict)
        self._move_optimizer_to_cpu()


class SequentialBlockProcessor:
    """
    –ü—Ä–æ—Ü–µ—Å—Å–æ—Ä –¥–ª—è –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏ –±–ª–æ–∫–æ–≤ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–∞.
    –¢–æ–ª—å–∫–æ –æ–¥–∏–Ω –±–ª–æ–∫ –Ω–∞—Ö–æ–¥–∏—Ç—Å—è –Ω–∞ GPU –≤ –∫–∞–∂–¥—ã–π –º–æ–º–µ–Ω—Ç –≤—Ä–µ–º–µ–Ω–∏.
    """
    
    def __init__(
        self,
        device: torch.device,
        cpu_device: torch.device = torch.device("cpu"),
        async_transfer: bool = True,
        prefetch_count: int = 1,
        debug: bool = False
    ):
        self.device = device
        self.cpu_device = cpu_device
        self.async_transfer = async_transfer
        self.prefetch_count = prefetch_count
        self.debug = debug
        
        self.stream = torch.cuda.Stream() if async_transfer and device.type == "cuda" else None
        self.executor = ThreadPoolExecutor(max_workers=2) if async_transfer else None
        
        self._timing_stats = {"to_gpu": [], "compute": [], "to_cpu": []}
    
    def process_blocks_forward(
        self,
        blocks: nn.ModuleList,
        hidden_states: torch.Tensor,
        *args,
        **kwargs
    ) -> torch.Tensor:
        """
        –ü–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –±–ª–æ–∫–∏ –≤–æ –≤—Ä–µ–º—è forward pass.
        –ö–∞–∂–¥—ã–π –±–ª–æ–∫ –∑–∞–≥—Ä—É–∂–∞–µ—Ç—Å—è –Ω–∞ GPU, –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç—Å—è, –∏ –≤—ã–≥—Ä—É–∂–∞–µ—Ç—Å—è.
        """
        for i, block in enumerate(blocks):
            if self.debug:
                start = time.perf_counter()
            
            # –ó–∞–≥—Ä—É–∂–∞–µ–º –±–ª–æ–∫ –Ω–∞ GPU
            block.to(self.device)
            
            if self.debug:
                torch.cuda.synchronize()
                load_time = time.perf_counter() - start
                self._timing_stats["to_gpu"].append(load_time)
                start = time.perf_counter()
            
            # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º
            hidden_states = block(hidden_states, *args, **kwargs)
            
            if self.debug:
                torch.cuda.synchronize()
                compute_time = time.perf_counter() - start
                self._timing_stats["compute"].append(compute_time)
                start = time.perf_counter()
            
            # –í—ã–≥—Ä—É–∂–∞–µ–º –±–ª–æ–∫ –æ–±—Ä–∞—Ç–Ω–æ –Ω–∞ CPU (–µ—Å–ª–∏ –Ω–µ –ø–æ—Å–ª–µ–¥–Ω–∏–π –∏ –Ω–µ –æ–±—É—á–∞–µ–º—ã–π)
            if not any(p.requires_grad for p in block.parameters()):
                block.to(self.cpu_device)
            
            if self.debug:
                torch.cuda.synchronize()
                unload_time = time.perf_counter() - start
                self._timing_stats["to_cpu"].append(unload_time)
        
        return hidden_states
    
    def get_timing_stats(self) -> Dict[str, float]:
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –≤—Ä–µ–º–µ–Ω–∏."""
        stats = {}
        for key, values in self._timing_stats.items():
            if values:
                stats[f"{key}_avg_ms"] = sum(values) / len(values) * 1000
                stats[f"{key}_total_ms"] = sum(values) * 1000
        return stats
    
    def clear_stats(self):
        """–û—á–∏—â–∞–µ—Ç —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É."""
        self._timing_stats = {"to_gpu": [], "compute": [], "to_cpu": []}


class VAESlicing:
    """–£—Ç–∏–ª–∏—Ç—ã –¥–ª—è —ç–∫–æ–Ω–æ–º–∏–∏ –ø–∞–º—è—Ç–∏ –ø—Ä–∏ —Ä–∞–±–æ—Ç–µ —Å VAE."""
    
    @staticmethod
    def encode_sliced(vae, images: torch.Tensor, slice_size: int = 1) -> torch.Tensor:
        """
        –ö–æ–¥–∏—Ä—É–µ—Ç –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –ø–æ —á–∞—Å—Ç—è–º –¥–ª—è —ç–∫–æ–Ω–æ–º–∏–∏ –ø–∞–º—è—Ç–∏.
        """
        latents = []
        for i in range(0, images.shape[0], slice_size):
            batch = images[i:i + slice_size]
            with torch.no_grad():
                latent = vae.encode(batch)
            latents.append(latent.cpu())
            
            if torch.cuda.is_available():
                torch.cuda.empty_cache()
        
        return torch.cat(latents, dim=0)
    
    @staticmethod
    def decode_sliced(vae, latents: torch.Tensor, slice_size: int = 1) -> torch.Tensor:
        """
        –î–µ–∫–æ–¥–∏—Ä—É–µ—Ç –ª–∞—Ç–µ–Ω—Ç—ã –ø–æ —á–∞—Å—Ç—è–º –¥–ª—è —ç–∫–æ–Ω–æ–º–∏–∏ –ø–∞–º—è—Ç–∏.
        """
        images = []
        for i in range(0, latents.shape[0], slice_size):
            batch = latents[i:i + slice_size]
            with torch.no_grad():
                image = vae.decode(batch.to(vae.device))
            images.append(image.cpu())
            
            if torch.cuda.is_available():
                torch.cuda.empty_cache()
        
        return torch.cat(images, dim=0)


class MemoryTracker:
    """–û—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏–µ –∏ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –ø–∞–º—è—Ç–∏."""
    
    def __init__(self, device: torch.device, log_interval: int = 100):
        self.device = device
        self.log_interval = log_interval
        self.step_count = 0
        self.peak_memory = 0
        self.memory_history = []
    
    def log_memory(self, tag: str = ""):
        """–õ–æ–≥–∏—Ä—É–µ—Ç —Ç–µ–∫—É—â–µ–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –ø–∞–º—è—Ç–∏."""
        if self.device.type != "cuda":
            return
        
        self.step_count += 1
        
        allocated = torch.cuda.memory_allocated(self.device) / 1024**3
        reserved = torch.cuda.memory_reserved(self.device) / 1024**3
        
        self.peak_memory = max(self.peak_memory, allocated)
        self.memory_history.append(allocated)
        
        if self.step_count % self.log_interval == 0:
            logger.info(
                f"[Memory {tag}] "
                f"Allocated: {allocated:.2f}GB, "
                f"Reserved: {reserved:.2f}GB, "
                f"Peak: {self.peak_memory:.2f}GB"
            )
    
    def get_stats(self) -> Dict[str, float]:
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –ø–∞–º—è—Ç–∏."""
        return {
            "peak_gb": self.peak_memory,
            "avg_gb": sum(self.memory_history) / len(self.memory_history) if self.memory_history else 0,
            "current_gb": self.memory_history[-1] if self.memory_history else 0,
        }
    
    @staticmethod
    def empty_cache():
        """–ê–≥—Ä–µ—Å—Å–∏–≤–Ω–æ –æ—á–∏—â–∞–µ—Ç –∫—ç—à CUDA."""
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
            torch.cuda.synchronize()
        gc.collect()


def setup_low_vram_training(
    config: LowVRAMConfig,
    model: nn.Module,
    text_encoders: List[nn.Module],
    vae: nn.Module,
    optimizer: torch.optim.Optimizer,
    device: torch.device
) -> Dict[str, Any]:
    """
    –ù–∞—Å—Ç—Ä–∞–∏–≤–∞–µ—Ç –æ–∫—Ä—É–∂–µ–Ω–∏–µ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è —Å –Ω–∏–∑–∫–∏–º VRAM.
    
    Returns:
        Dict —Å –º–æ–¥–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω—ã–º–∏ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–∞–º–∏ –∏ —É—Ç–∏–ª–∏—Ç–∞–º–∏.
    """
    result = {
        "model": model,
        "text_encoders": text_encoders,
        "vae": vae,
        "optimizer": optimizer,
        "memory_tracker": MemoryTracker(device),
    }
    
    logger.info(f"Setting up low VRAM training with strategy: {config.strategy.value}")
    logger.info(f"Available VRAM: {config.available_vram_gb}GB, RAM: {config.available_ram_gb}GB")
    
    # 1. Gradient Checkpointing
    if config.gradient_checkpointing:
        if hasattr(model, "enable_gradient_checkpointing"):
            model.enable_gradient_checkpointing(cpu_offload=config.cpu_offload_checkpointing)
            logger.info("Enabled gradient checkpointing for model")
    
    # 2. Text Encoder Offloading
    if config.text_encoder_offload_to_cpu:
        for i, te in enumerate(text_encoders):
            if te is not None:
                te.to("cpu")
                logger.info(f"Moved text_encoder[{i}] to CPU")
    
    # 3. VAE Offloading
    if config.vae_offload:
        vae.to("cpu")
        logger.info("Moved VAE to CPU")
    
    # 4. Optimizer Offloading
    if config.optimizer_offload_to_cpu:
        result["optimizer"] = CPUOffloadOptimizer(
            optimizer, device, 
            pin_memory=config.optimizer_pin_memory
        )
        logger.info("Wrapped optimizer with CPU offloading")
    
    # 5. Channels Last (–¥–ª—è –Ω–µ–∫–æ—Ç–æ—Ä—ã—Ö –æ–ø–µ—Ä–∞—Ü–∏–π –±—ã—Å—Ç—Ä–µ–µ)
    if config.use_channels_last and hasattr(model, "to"):
        try:
            model = model.to(memory_format=torch.channels_last)
            logger.info("Converted model to channels_last memory format")
        except Exception as e:
            logger.warning(f"Could not convert to channels_last: {e}")
    
    # 6. Sequential Block Processor
    result["block_processor"] = SequentialBlockProcessor(
        device=device,
        async_transfer=config.swap_async,
        debug=False
    )
    
    # –í—ã–≤–æ–¥–∏–º –æ—Ü–µ–Ω–∫—É –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –ø–∞–º—è—Ç–∏
    # –î–ª—è Flux.2 Klein 9B
    mem_estimate = config.estimate_memory_usage(9.0)
    logger.info(f"Estimated memory usage for 9B model: {mem_estimate}")
    
    return result


def get_optimal_config_for_vram(vram_gb: float, ram_gb: float) -> LowVRAMConfig:
    """
    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –æ–ø—Ç–∏–º–∞–ª—å–Ω—É—é –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é –¥–ª—è –∑–∞–¥–∞–Ω–Ω–æ–≥–æ –æ–±—ä–µ–º–∞ VRAM.
    """
    if vram_gb >= 24:
        return LowVRAMConfig(
            strategy=OffloadStrategy.NONE,
            available_vram_gb=vram_gb,
            available_ram_gb=ram_gb,
            blocks_to_swap=0,
            gradient_checkpointing=True,
            cpu_offload_checkpointing=False,
            use_fp8_base=False,
            optimizer_offload_to_cpu=False,
        )
    elif vram_gb >= 16:
        return LowVRAMConfig(
            strategy=OffloadStrategy.CONSERVATIVE,
            available_vram_gb=vram_gb,
            available_ram_gb=ram_gb,
            blocks_to_swap=10,
            gradient_checkpointing=True,
            cpu_offload_checkpointing=False,
            use_fp8_base=True,
            optimizer_offload_to_cpu=False,
        )
    elif vram_gb >= 8:
        return LowVRAMConfig(
            strategy=OffloadStrategy.AGGRESSIVE,
            available_vram_gb=vram_gb,
            available_ram_gb=ram_gb,
            blocks_to_swap=25,
            gradient_checkpointing=True,
            cpu_offload_checkpointing=True,
            use_fp8_base=True,
            optimizer_offload_to_cpu=True,
            effective_batch_size=1,
            gradient_accumulation_steps=8,
        )
    else:
        return LowVRAMConfig(
            strategy=OffloadStrategy.EXTREME,
            available_vram_gb=vram_gb,
            available_ram_gb=ram_gb,
            blocks_to_swap=35,  # –ü–æ—á—Ç–∏ –≤—Å–µ –±–ª–æ–∫–∏
            gradient_checkpointing=True,
            cpu_offload_checkpointing=True,
            use_fp8_base=True,
            optimizer_offload_to_cpu=True,
            empty_cache_frequently=True,
            effective_batch_size=1,
            gradient_accumulation_steps=16,
        )


# –£—Ç–∏–ª–∏—Ç–Ω—ã–µ —Ñ—É–Ω–∫—Ü–∏–∏ –¥–ª—è –±—ã—Å—Ç—Ä–æ–≥–æ –¥–æ—Å—Ç—É–ø–∞
def aggressive_memory_cleanup(device: Optional[torch.device] = None):
    """–ê–≥—Ä–µ—Å—Å–∏–≤–Ω–∞—è –æ—á–∏—Å—Ç–∫–∞ –ø–∞–º—è—Ç–∏."""
    gc.collect()
    if torch.cuda.is_available():
        if device is not None:
            torch.cuda.empty_cache()
            clean_memory_on_device(device)
        else:
            torch.cuda.empty_cache()
            for i in range(torch.cuda.device_count()):
                with torch.cuda.device(i):
                    torch.cuda.empty_cache()


def estimate_trainable_params_memory(model: nn.Module) -> float:
    """–û—Ü–µ–Ω–∏–≤–∞–µ—Ç –ø–∞–º—è—Ç—å –¥–ª—è –æ–±—É—á–∞–µ–º—ã—Ö –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –≤ GB."""
    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
    # –î–ª—è Adam: param + grad + 2 –º–æ–º–µ–Ω—Ç–∞ = 4x —Ä–∞–∑–º–µ—Ä –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤
    # –í FP32: 4 bytes per param
    bytes_needed = trainable_params * 4 * 4  # 4 copies, 4 bytes each
    return bytes_needed / (1024 ** 3)


# =============================================================================
# VRAM SAFETY CHECKER - –ü—Ä–µ–¥–∏–∫—Ç–∏–≤–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ –ø–∞–º—è—Ç–∏
# =============================================================================
@dataclass
class VRAMEstimate:
    """–†–µ–∑—É–ª—å—Ç–∞—Ç –æ—Ü–µ–Ω–∫–∏ VRAM."""
    total_vram_needed_gb: float
    base_model_gb: float
    lora_weights_gb: float
    activations_peak_gb: float
    optimizer_gb: float
    text_encoder_gb: float
    safety_margin_gb: float
    
    available_vram_gb: float
    will_fit: bool
    risk_level: str  # "safe", "warning", "danger", "critical"
    recommendations: List[str] = field(default_factory=list)
    
    def __str__(self) -> str:
        status = "‚úÖ" if self.will_fit else "‚ö†Ô∏è"
        return (
            f"{status} VRAM Estimate: {self.total_vram_needed_gb:.1f}GB needed, "
            f"{self.available_vram_gb:.1f}GB available ({self.risk_level})"
        )


def estimate_vram_usage(
    model_params_billions: float = 9.0,
    resolution: Tuple[int, int] = (1024, 1024),
    batch_size: int = 1,
    network_dim: int = 16,
    gradient_checkpointing: bool = True,
    use_fp8_base: bool = True,
    optimizer_offload: bool = True,
    cache_text_encoder: bool = True,
    blocks_to_swap: int = 20,
    available_vram_gb: float = 8.0,
) -> VRAMEstimate:
    """
    –ü—Ä–µ–¥–∏–∫—Ç–∏–≤–Ω–æ –æ—Ü–µ–Ω–∏–≤–∞–µ—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ VRAM –ø–µ—Ä–µ–¥ –Ω–∞—á–∞–ª–æ–º –æ–±—É—á–µ–Ω–∏—è.
    
    –ü–æ–º–æ–≥–∞–µ—Ç –ø—Ä–µ–¥–æ—Ç–≤—Ä–∞—Ç–∏—Ç—å OOM (Out of Memory) –æ—à–∏–±–∫–∏ –∑–∞—Ä–∞–Ω–µ–µ.
    
    Args:
        model_params_billions: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –º–æ–¥–µ–ª–∏ –≤ –º–∏–ª–ª–∏–∞—Ä–¥–∞—Ö (9B –¥–ª—è Klein, 12B –¥–ª—è Dev)
        resolution: –†–∞–∑–º–µ—Ä –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è (width, height)
        batch_size: –†–∞–∑–º–µ—Ä –±–∞—Ç—á–∞
        network_dim: –†–∞–Ω–≥ LoRA
        gradient_checkpointing: –í–∫–ª—é—á–µ–Ω –ª–∏ gradient checkpointing
        use_fp8_base: –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –ª–∏ FP8 –¥–ª—è –±–∞–∑–æ–≤–æ–π –º–æ–¥–µ–ª–∏
        optimizer_offload: –í—ã–≥—Ä—É–∂–∞–µ—Ç—Å—è –ª–∏ –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä –Ω–∞ CPU
        cache_text_encoder: –ö—ç—à–∏—Ä—É—é—Ç—Å—è –ª–∏ –≤—ã—Ö–æ–¥—ã text encoder
        blocks_to_swap: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –±–ª–æ–∫–æ–≤ –¥–ª—è CPU swapping
        available_vram_gb: –î–æ—Å—Ç—É–ø–Ω–∞—è VRAM –≤ GB
    
    Returns:
        VRAMEstimate —Å –¥–µ—Ç–∞–ª—å–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–µ–π
    """
    recommendations = []
    
    # === –ë–∞–∑–æ–≤–∞—è –º–æ–¥–µ–ª—å ===
    # FP8: 1 byte/param, FP16: 2 bytes/param
    bytes_per_param = 1.0 if use_fp8_base else 2.0
    base_model_bytes = model_params_billions * 1e9 * bytes_per_param
    
    # –£—á–∏—Ç—ã–≤–∞–µ–º block swapping - —á–∞—Å—Ç—å –º–æ–¥–µ–ª–∏ –Ω–∞ CPU
    if blocks_to_swap > 0:
        total_blocks = 57  # Flux –∏–º–µ–µ—Ç –ø—Ä–∏–º–µ—Ä–Ω–æ 57 –±–ª–æ–∫–æ–≤ (19 double + 38 single)
        gpu_blocks_ratio = max(0.1, 1.0 - (blocks_to_swap / total_blocks))
        base_model_bytes *= gpu_blocks_ratio
    
    base_model_gb = base_model_bytes / (1024 ** 3)
    
    # === LoRA –≤–µ—Å–∞ ===
    # LoRA –¥–æ–±–∞–≤–ª—è–µ—Ç –ø—Ä–∏–º–µ—Ä–Ω–æ (rank * hidden_dim * 2) –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –Ω–∞ —Å–ª–æ–π
    # –ü—Ä–∏–º–µ—Ä–Ω–∞—è –æ—Ü–µ–Ω–∫–∞: rank * 0.01 GB
    lora_weights_gb = network_dim * 0.008  # ~130MB –¥–ª—è rank 16
    
    # === –ê–∫—Ç–∏–≤–∞—Ü–∏–∏ ===
    # –†–∞–∑–º–µ—Ä –∞–∫—Ç–∏–≤–∞—Ü–∏–π –∑–∞–≤–∏—Å–∏—Ç –æ—Ç —Ä–∞–∑—Ä–µ—à–µ–Ω–∏—è –∏ batch size
    pixels = resolution[0] * resolution[1]
    # Latent space 1/8 –æ—Ç –æ—Ä–∏–≥–∏–Ω–∞–ª–∞, 16 –∫–∞–Ω–∞–ª–æ–≤, FP16
    latent_size = (pixels / 64) * 16 * 2  # bytes
    
    # –ê–∫—Ç–∏–≤–∞—Ü–∏–∏ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–∞ –ø—Ä–∏–º–µ—Ä–Ω–æ 4KB –Ω–∞ –ø–∏–∫—Å–µ–ª—å latent
    if gradient_checkpointing:
        # –° checkpointing —Ö—Ä–∞–Ω–∏–º —Ç–æ–ª—å–∫–æ 1/4 –∞–∫—Ç–∏–≤–∞—Ü–∏–π
        activation_multiplier = 0.5
    else:
        activation_multiplier = 4.0
    
    activations_bytes = latent_size * activation_multiplier * batch_size * 1000
    activations_peak_gb = activations_bytes / (1024 ** 3)
    
    # === Text Encoder ===
    if cache_text_encoder:
        # –ï—Å–ª–∏ –∫—ç—à–∏—Ä—É–µ–º, text encoder –Ω–µ –∑–∞–Ω–∏–º–∞–µ—Ç VRAM –≤–æ –≤—Ä–µ–º—è –æ–±—É—á–µ–Ω–∏—è
        text_encoder_gb = 0.0
    else:
        # T5-XXL ~10GB, CLIP-L ~0.5GB
        text_encoder_gb = 2.0  # –ú–∏–Ω–∏–º—É–º –¥–ª—è inference
    
    # === Optimizer ===
    if optimizer_offload:
        optimizer_gb = 0.0
    else:
        # AdamW: 2 –º–æ–º–µ–Ω—Ç–∞ * —Ä–∞–∑–º–µ—Ä LoRA –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ * 4 bytes
        lora_params = network_dim * 1e6  # –ü—Ä–∏–º–µ—Ä–Ω–∞—è –æ—Ü–µ–Ω–∫–∞
        optimizer_gb = (lora_params * 2 * 4) / (1024 ** 3)
    
    # === Safety margin ===
    # CUDA –∏ PyTorch —Ç—Ä–µ–±—É—é—Ç –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—É—é –ø–∞–º—è—Ç—å –¥–ª—è operations
    safety_margin_gb = 1.0 + (batch_size * 0.2)
    
    # === Total ===
    total_vram_needed_gb = (
        base_model_gb + 
        lora_weights_gb + 
        activations_peak_gb + 
        optimizer_gb + 
        text_encoder_gb + 
        safety_margin_gb
    )
    
    # === Risk assessment ===
    headroom = available_vram_gb - total_vram_needed_gb
    
    if headroom >= 2.0:
        risk_level = "safe"
        will_fit = True
    elif headroom >= 0.5:
        risk_level = "warning"
        will_fit = True
        recommendations.append("‚ö†Ô∏è –ë–ª–∏–∑–∫–æ –∫ –ª–∏–º–∏—Ç—É VRAM. –ú–æ–≥—É—Ç –±—ã—Ç—å —Å–ø–∞–π–∫–∏ –ø–∞–º—è—Ç–∏.")
    elif headroom >= 0:
        risk_level = "danger"
        will_fit = True
        recommendations.append("üî¥ –û—á–µ–Ω—å –º–∞–ª–æ –∑–∞–ø–∞—Å–∞. –í—ã—Å–æ–∫–∏–π —Ä–∏—Å–∫ OOM –ø—Ä–∏ –ø–∏–∫–∞—Ö –Ω–∞–≥—Ä—É–∑–∫–∏.")
        recommendations.append("–†–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è —É–≤–µ–ª–∏—á–∏—Ç—å blocks_to_swap –∏–ª–∏ —É–º–µ–Ω—å—à–∏—Ç—å batch_size.")
    else:
        risk_level = "critical"
        will_fit = False
        recommendations.append("‚ùå –ù–ï–î–û–°–¢–ê–¢–û–ß–ù–û VRAM! –û–±—É—á–µ–Ω–∏–µ –≤—ã–∑–æ–≤–µ—Ç OOM –æ—à–∏–±–∫—É.")
        
        # –ü—Ä–µ–¥–ª–∞–≥–∞–µ–º —Ä–µ—à–µ–Ω–∏—è
        if not use_fp8_base:
            recommendations.append("‚Üí –í–∫–ª—é—á–∏—Ç–µ use_fp8_base –¥–ª—è —ç–∫–æ–Ω–æ–º–∏–∏ ~50% VRAM –º–æ–¥–µ–ª–∏")
        if not gradient_checkpointing:
            recommendations.append("‚Üí –í–∫–ª—é—á–∏—Ç–µ gradient_checkpointing –¥–ª—è —ç–∫–æ–Ω–æ–º–∏–∏ –∞–∫—Ç–∏–≤–∞—Ü–∏–π")
        if blocks_to_swap < 25:
            recommendations.append(f"‚Üí –£–≤–µ–ª–∏—á—å—Ç–µ blocks_to_swap –¥–æ {min(35, blocks_to_swap + 10)}")
        if batch_size > 1:
            recommendations.append("‚Üí –£–º–µ–Ω—å—à–∏—Ç–µ batch_size –¥–æ 1")
        if not optimizer_offload:
            recommendations.append("‚Üí –í–∫–ª—é—á–∏—Ç–µ optimizer_offload –¥–ª—è –≤—ã–≥—Ä—É–∑–∫–∏ –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä–∞ –Ω–∞ CPU")
        if resolution[0] > 512 or resolution[1] > 512:
            recommendations.append("‚Üí –ü–æ–ø—Ä–æ–±—É–π—Ç–µ —É–º–µ–Ω—å—à–∏—Ç—å —Ä–∞–∑—Ä–µ—à–µ–Ω–∏–µ –æ–±—É—á–µ–Ω–∏—è")
    
    return VRAMEstimate(
        total_vram_needed_gb=total_vram_needed_gb,
        base_model_gb=base_model_gb,
        lora_weights_gb=lora_weights_gb,
        activations_peak_gb=activations_peak_gb,
        optimizer_gb=optimizer_gb,
        text_encoder_gb=text_encoder_gb,
        safety_margin_gb=safety_margin_gb,
        available_vram_gb=available_vram_gb,
        will_fit=will_fit,
        risk_level=risk_level,
        recommendations=recommendations,
    )


def print_vram_estimate(estimate: VRAMEstimate):
    """–í—ã–≤–æ–¥–∏—Ç –¥–µ—Ç–∞–ª—å–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ–± –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–∏ VRAM."""
    logger.info("=" * 60)
    logger.info("  VRAM Usage Estimate")
    logger.info("=" * 60)
    logger.info(f"  Base Model:      {estimate.base_model_gb:>6.2f} GB")
    logger.info(f"  LoRA Weights:    {estimate.lora_weights_gb:>6.2f} GB")
    logger.info(f"  Activations:     {estimate.activations_peak_gb:>6.2f} GB")
    logger.info(f"  Text Encoder:    {estimate.text_encoder_gb:>6.2f} GB")
    logger.info(f"  Optimizer:       {estimate.optimizer_gb:>6.2f} GB")
    logger.info(f"  Safety Margin:   {estimate.safety_margin_gb:>6.2f} GB")
    logger.info("-" * 60)
    logger.info(f"  TOTAL NEEDED:    {estimate.total_vram_needed_gb:>6.2f} GB")
    logger.info(f"  AVAILABLE:       {estimate.available_vram_gb:>6.2f} GB")
    logger.info("-" * 60)
    
    status_icons = {
        "safe": "‚úÖ SAFE",
        "warning": "‚ö†Ô∏è  WARNING",
        "danger": "üî¥ DANGER",
        "critical": "‚ùå CRITICAL"
    }
    logger.info(f"  Status: {status_icons.get(estimate.risk_level, estimate.risk_level)}")
    
    for rec in estimate.recommendations:
        logger.info(f"  {rec}")
    
    logger.info("=" * 60)


# =============================================================================
# AUTO-RESUME TRAINING - –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ –ø—Ä–æ–¥–æ–ª–∂–µ–Ω–∏–µ –æ–±—É—á–µ–Ω–∏—è
# =============================================================================
def find_latest_checkpoint(output_dir: str) -> Optional[str]:
    """
    –ù–∞—Ö–æ–¥–∏—Ç –ø–æ—Å–ª–µ–¥–Ω–∏–π —á–µ–∫–ø–æ–∏–Ω—Ç –≤ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏ –≤—ã–≤–æ–¥–∞.
    
    –ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç —Ñ–æ—Ä–º–∞—Ç—ã:
    - flux2_lora_step_1000.safetensors
    - epoch_5_step_500.safetensors
    - *.safetensors (–ø–æ –≤—Ä–µ–º–µ–Ω–∏ –º–æ–¥–∏—Ñ–∏–∫–∞—Ü–∏–∏)
    
    Args:
        output_dir: –î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è —Å —á–µ–∫–ø–æ–∏–Ω—Ç–∞–º–∏
    
    Returns:
        –ü—É—Ç—å –∫ –ø–æ—Å–ª–µ–¥–Ω–µ–º—É —á–µ–∫–ø–æ–∏–Ω—Ç—É –∏–ª–∏ None
    """
    import glob
    import re
    from pathlib import Path
    
    output_path = Path(output_dir)
    
    if not output_path.exists():
        return None
    
    # –ò—â–µ–º safetensors —Ñ–∞–π–ª—ã
    checkpoints = list(output_path.glob("*.safetensors"))
    
    if not checkpoints:
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –ø–æ–¥–ø–∞–ø–∫–∏
        checkpoints = list(output_path.glob("**/*.safetensors"))
    
    if not checkpoints:
        return None
    
    # –ü—Ä–æ–±—É–µ–º –æ—Ç—Å–æ—Ä—Ç–∏—Ä–æ–≤–∞—Ç—å –ø–æ –Ω–æ–º–µ—Ä—É —à–∞–≥–∞
    step_pattern = re.compile(r'step[_-]?(\d+)', re.IGNORECASE)
    epoch_pattern = re.compile(r'epoch[_-]?(\d+)', re.IGNORECASE)
    
    def extract_order(path: Path) -> Tuple[int, int, float]:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –ø–æ—Ä—è–¥–æ–∫ —Å–æ—Ä—Ç–∏—Ä–æ–≤–∫–∏ (epoch, step, mtime)."""
        name = path.name
        
        epoch = 0
        step = 0
        
        epoch_match = epoch_pattern.search(name)
        if epoch_match:
            epoch = int(epoch_match.group(1))
        
        step_match = step_pattern.search(name)
        if step_match:
            step = int(step_match.group(1))
        
        mtime = path.stat().st_mtime
        
        return (epoch, step, mtime)
    
    # –°–æ—Ä—Ç–∏—Ä—É–µ–º: epoch DESC, step DESC, mtime DESC
    sorted_checkpoints = sorted(checkpoints, key=extract_order, reverse=True)
    
    if sorted_checkpoints:
        latest = str(sorted_checkpoints[0])
        logger.info(f"üîÑ Found latest checkpoint: {latest}")
        return latest
    
    return None


def auto_resume_training(
    output_dir: str,
    args,
    force: bool = False
) -> bool:
    """
    –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –Ω–∞—Å—Ç—Ä–∞–∏–≤–∞–µ—Ç –ø—Ä–æ–¥–æ–ª–∂–µ–Ω–∏–µ –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ—Å–ª–µ–¥–Ω–µ–≥–æ —á–µ–∫–ø–æ–∏–Ω—Ç–∞.
    
    Args:
        output_dir: –î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è —Å —á–µ–∫–ø–æ–∏–Ω—Ç–∞–º–∏
        args: Namespace —Å –∞—Ä–≥—É–º–µ–Ω—Ç–∞–º–∏ –æ–±—É—á–µ–Ω–∏—è
        force: –§–æ—Ä—Å–∏—Ä–æ–≤–∞—Ç—å resume –¥–∞–∂–µ –µ—Å–ª–∏ —É–∂–µ —É–∫–∞–∑–∞–Ω
    
    Returns:
        True –µ—Å–ª–∏ resume –Ω–∞—Å—Ç—Ä–æ–µ–Ω, False –µ—Å–ª–∏ —á–µ–∫–ø–æ–∏–Ω—Ç –Ω–µ –Ω–∞–π–¥–µ–Ω
    """
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –Ω–µ —É–∫–∞–∑–∞–Ω –ª–∏ —É–∂–µ resume
    if hasattr(args, 'resume') and args.resume and not force:
        logger.info(f"Resume already configured: {args.resume}")
        return True
    
    if hasattr(args, 'network_weights') and args.network_weights and not force:
        logger.info(f"Network weights already specified: {args.network_weights}")
        return True
    
    # –ò—â–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–π —á–µ–∫–ø–æ–∏–Ω—Ç
    latest = find_latest_checkpoint(output_dir)
    
    if latest:
        logger.info(f"üîÑ Auto-resume: Found checkpoint at {latest}")
        
        # –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º network_weights –¥–ª—è –ø—Ä–æ–¥–æ–ª–∂–µ–Ω–∏—è LoRA –æ–±—É—á–µ–Ω–∏—è
        if not hasattr(args, 'network_weights') or not args.network_weights:
            args.network_weights = latest
            logger.info(f"   Set network_weights = {latest}")
        
        return True
    
    logger.info("No previous checkpoint found. Starting fresh training.")
    return False


def get_training_progress(output_dir: str) -> Dict[str, Any]:
    """
    –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç –ø—Ä–æ–≥—Ä–µ—Å—Å –æ–±—É—á–µ–Ω–∏—è –∏–∑ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏.
    
    Returns:
        Dict —Å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–µ–π –æ –ø—Ä–æ–≥—Ä–µ—Å—Å–µ:
        - total_checkpoints: –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —á–µ–∫–ø–æ–∏–Ω—Ç–æ–≤
        - latest_checkpoint: –ø—É—Ç—å –∫ –ø–æ—Å–ª–µ–¥–Ω–µ–º—É
        - latest_step: –Ω–æ–º–µ—Ä –ø–æ—Å–ª–µ–¥–Ω–µ–≥–æ —à–∞–≥–∞ (–µ—Å–ª–∏ –æ–ø—Ä–µ–¥–µ–ª–∏–º)
        - training_started: –±—ã–ª–∞ –ª–∏ –Ω–∞—á–∞—Ç–∞ —Ç—Ä–µ–Ω–∏—Ä–æ–≤–∫–∞
    """
    import re
    from pathlib import Path
    
    output_path = Path(output_dir)
    result = {
        "total_checkpoints": 0,
        "latest_checkpoint": None,
        "latest_step": 0,
        "latest_epoch": 0,
        "training_started": False,
    }
    
    if not output_path.exists():
        return result
    
    checkpoints = list(output_path.glob("**/*.safetensors"))
    result["total_checkpoints"] = len(checkpoints)
    result["training_started"] = len(checkpoints) > 0
    
    if checkpoints:
        result["latest_checkpoint"] = find_latest_checkpoint(output_dir)
        
        # –ü—Ä–æ–±—É–µ–º –∏–∑–≤–ª–µ—á—å step –∏–∑ –∏–º–µ–Ω–∏
        if result["latest_checkpoint"]:
            name = Path(result["latest_checkpoint"]).name
            step_match = re.search(r'step[_-]?(\d+)', name, re.IGNORECASE)
            if step_match:
                result["latest_step"] = int(step_match.group(1))
            
            epoch_match = re.search(r'epoch[_-]?(\d+)', name, re.IGNORECASE)
            if epoch_match:
                result["latest_epoch"] = int(epoch_match.group(1))
    
    return result
