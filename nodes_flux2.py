# -*- coding: utf-8 -*-
"""
ComfyUI Nodes for Flux.2 Training
==================================

–ù–æ–¥—ã –¥–ª—è –æ–±—É—á–µ–Ω–∏—è LoRA –Ω–∞ –º–æ–¥–µ–ª—è—Ö Flux.2 (Klein 9B –∏ Dev) 
—Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π low VRAM GPU (8GB –∏ –º–µ–Ω–µ–µ).

–û—Å–æ–±–µ–Ω–Ω–æ—Å—Ç–∏:
- –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –≤–µ—Ä—Å–∏–∏ –º–æ–¥–µ–ª–∏
- –ù–∞—Å—Ç—Ä–∞–∏–≤–∞–µ–º—ã–µ —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏ —ç–∫–æ–Ω–æ–º–∏–∏ –ø–∞–º—è—Ç–∏
- –ü–æ–¥–¥–µ—Ä–∂–∫–∞ CPU offloading
- –ü–æ–ª–Ω–∞—è –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è —Å —Å—É—â–µ—Å—Ç–≤—É—é—â–µ–π —Å–∏—Å—Ç–µ–º–æ–π FluxTrainer
- LAZY IMPORTS - –Ω–æ–¥—ã –∑–∞–≥—Ä—É–∂–∞—é—Ç—Å—è –¥–∞–∂–µ –µ—Å–ª–∏ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –Ω–µ–¥–æ—Å—Ç—É–ø–Ω—ã

Author: ComfyUI-FluxTrainer-Pro Team
License: Apache-2.0
"""

import os
import sys
import json
import shlex
import shutil
import time
import logging
from pathlib import Path
from typing import Dict, Any, Optional, Tuple, Callable

import folder_paths
import comfy.model_management as mm
import comfy.utils

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–≥–µ—Ä–∞
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

script_directory = os.path.dirname(os.path.abspath(__file__))

# =============================================================================
# LAZY IMPORT SYSTEM - –ö–ª—é—á–µ–≤–æ–µ —Ä–µ—à–µ–Ω–∏–µ –¥–ª—è —Å—Ç–∞–±–∏–ª—å–Ω–æ–π –∑–∞–≥—Ä—É–∑–∫–∏
# =============================================================================
# –ö—ç—à –¥–ª—è –∑–∞–≥—Ä—É–∂–µ–Ω–Ω—ã—Ö –º–æ–¥—É–ª–µ–π - –∑–∞–≥—Ä—É–∂–∞–µ–º –æ–¥–∏–Ω —Ä–∞–∑ –ø—Ä–∏ –ø–µ—Ä–≤–æ–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–∏
_CACHED_MODULES: Dict[str, Any] = {}
_IMPORT_ERROR: Optional[str] = None


def _lazy_import_flux_utils():
    """–ó–∞–≥—Ä—É–∂–∞–µ—Ç —Ç–æ–ª—å–∫–æ flux_utils –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –º–æ–¥–µ–ª–µ–π (–ª–µ–≥–∫–∞—è –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç—å)."""
    if "flux_utils" in _CACHED_MODULES:
        return _CACHED_MODULES["flux_utils"]
    
    try:
        from .library import flux_utils
        _CACHED_MODULES["flux_utils"] = flux_utils
        return flux_utils
    except Exception as e:
        logger.warning(f"Could not import flux_utils: {e}")
        return None


def _lazy_import_training():
    """
    –ó–∞–≥—Ä—É–∂–∞–µ—Ç —Ç—è–∂–µ–ª—ã–µ –º–æ–¥—É–ª–∏ –æ–±—É—á–µ–Ω–∏—è –¢–û–õ–¨–ö–û –∫–æ–≥–¥–∞ –æ–Ω–∏ —Ä–µ–∞–ª—å–Ω–æ –Ω—É–∂–Ω—ã.
    –≠—Ç–æ –ø—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–∞–µ—Ç –ø–∞–¥–µ–Ω–∏–µ ComfyUI –ø—Ä–∏ —Å—Ç–∞—Ä—Ç–µ –∏–∑-–∑–∞ –æ—à–∏–±–æ–∫ –≤ diffusers/triton/bitsandbytes.
    
    –û—à–∏–±–∫–∞ –ø–æ—è–≤–∏—Ç—Å—è –¢–û–õ–¨–ö–û –ø—Ä–∏ –Ω–∞–∂–∞—Ç–∏–∏ Queue Prompt, –∞ –Ω–µ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ –Ω–æ–¥!
    
    NOTE: –ü–∞—Ç—á triton –≤—ã–ø–æ–ª–Ω—è–µ—Ç—Å—è –≤ __init__.py (–≥–ª–æ–±–∞–ª—å–Ω–æ) –¥–æ –∑–∞–≥—Ä—É–∑–∫–∏ –ª—é–±—ã—Ö –Ω–æ–¥.
    """
    global _IMPORT_ERROR
    
    if "FluxNetworkTrainer" in _CACHED_MODULES:
        return _CACHED_MODULES
    
    # =======================================================================
    # WINDOWS ENVIRONMENT SETUP - –ü–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –æ–∫—Ä—É–∂–µ–Ω–∏—è –¥–ª—è Triton/CUDA
    # –°–∞–º –ø–∞—Ç—á triton —É–∂–µ –≤—ã–ø–æ–ª–Ω–µ–Ω –≤ __init__.py
    # =======================================================================
    if sys.platform == 'win32':
        python_home = os.path.dirname(sys.executable)
        
        # –î–æ–±–∞–≤–ª—è–µ–º –ø—É—Ç—å –∫ include –¥–ª—è Triton
        include_path = os.path.join(python_home, 'include')
        if os.path.exists(include_path):
            os.environ.setdefault('INCLUDE', include_path)
        
        # –ü—É—Ç—å –∫ ptxas.exe –¥–ª—è CUDA –∫–æ–º–ø–∏–ª—è—Ü–∏–∏ (–µ—Å–ª–∏ –µ—Å—Ç—å)
        ptxas_candidates = [
            os.path.join(python_home, 'Library', 'bin', 'ptxas.exe'),
            os.path.join(os.environ.get('CUDA_PATH', ''), 'bin', 'ptxas.exe'),
        ]
        for ptxas_path in ptxas_candidates:
            if os.path.exists(ptxas_path):
                os.environ['TRITON_PTXAS_PATH'] = ptxas_path
                break
        
        # –û—Ç–∫–ª—é—á–∞–µ–º JIT –∫–æ–º–ø–∏–ª—è—Ü–∏—é Triton –µ—Å–ª–∏ –Ω–µ—Ç –∫–æ–º–ø–∏–ª—è—Ç–æ—Ä–∞
        if not os.path.exists(os.path.join(python_home, 'include', 'Python.h')):
            os.environ.setdefault('TRITON_DISABLE_LINE_INFO', '1')
            logger.debug("[Flux2] Windows Embedded Python detected")
    
    try:
        import toml
        import torch
        from .flux_train_network_comfy import FluxNetworkTrainer
        from .train_network import setup_parser as train_network_setup_parser
        from .library import flux_train_utils, flux_utils, train_util
        from .library.low_vram_utils import (
            LowVRAMConfig, 
            OffloadStrategy, 
            get_optimal_config_for_vram,
            aggressive_memory_cleanup,
            estimate_vram_usage,
            print_vram_estimate,
            auto_resume_training,
            get_training_progress,
            find_latest_checkpoint,
        )
        
        # IPEX (Intel GPU) - —Å—Ç—Ä–æ–≥–æ –æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ
        clean_memory_on_device: Callable = lambda *args, **kwargs: None
        try:
            from .library.device_utils import init_ipex, clean_memory_on_device as _clean
            init_ipex()
            clean_memory_on_device = _clean
        except ImportError:
            pass
        except Exception as ipex_err:
            logger.debug(f"IPEX not available: {ipex_err}")
        
        _CACHED_MODULES.update({
            "toml": toml,
            "torch": torch,
            "FluxNetworkTrainer": FluxNetworkTrainer,
            "train_network_setup_parser": train_network_setup_parser,
            "flux_train_utils": flux_train_utils,
            "flux_utils": flux_utils,
            "train_util": train_util,
            "LowVRAMConfig": LowVRAMConfig,
            "OffloadStrategy": OffloadStrategy,
            "get_optimal_config_for_vram": get_optimal_config_for_vram,
            "aggressive_memory_cleanup": aggressive_memory_cleanup,
            "estimate_vram_usage": estimate_vram_usage,
            "print_vram_estimate": print_vram_estimate,
            "auto_resume_training": auto_resume_training,
            "get_training_progress": get_training_progress,
            "find_latest_checkpoint": find_latest_checkpoint,
            "clean_memory_on_device": clean_memory_on_device,
        })
        
        logger.info("[Flux2] Training modules loaded successfully")
        return _CACHED_MODULES
        
    except Exception as e:
        _IMPORT_ERROR = str(e)
        import traceback
        # sys already imported at module level (line 21)
        
        # –î–µ—Ç–∞–ª—å–Ω–∞—è –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∞ –æ—à–∏–±–∫–∏
        error_lower = str(e).lower()
        traceback_str = traceback.format_exc()
        
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ç–∏–ø –æ—à–∏–±–∫–∏ –∏ –¥–∞—ë–º –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–µ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏
        if "python.h" in error_lower or "include file" in error_lower:
            problem = "[ERROR] COMPILATION ERROR: Python.h not found"
            diagnosis = [
                "–í—ã –∏—Å–ø–æ–ª—å–∑—É–µ—Ç–µ embedded/portable Python, –∫–æ—Ç–æ—Ä—ã–π –Ω–µ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç –∫–æ–º–ø–∏–ª—è—Ü–∏—é C —Ä–∞—Å—à–∏—Ä–µ–Ω–∏–π.",
                "",
                "üîß –†–ï–®–ï–ù–ò–ï:",
                "1. –ó–∞–ø—É—Å—Ç–∏—Ç–µ: python install.py",
                "   –≠—Ç–æ —É—Å—Ç–∞–Ω–æ–≤–∏—Ç pre-built wheels –¥–ª—è triton –∏ bitsandbytes",
                "",
                "2. –ò–ª–∏ —É—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ –ø–æ–ª–Ω—ã–π Python —Å python.org:",
                "   - –°–∫–∞—á–∞–π—Ç–µ 'Windows installer (64-bit)' —Å https://python.org",
                "   - –ü—Ä–∏ —É—Å—Ç–∞–Ω–æ–≤–∫–µ –≤—ã–±–µ—Ä–∏—Ç–µ 'Add Python to PATH'",
                "   - –ü–µ—Ä–µ—É—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ ComfyUI —Å –ø–æ–ª–Ω—ã–º Python",
            ]
        elif "triton" in error_lower:
            problem = "[ERROR] TRITON ERROR: Could not load triton"
            diagnosis = [
                "Triton —Ç—Ä–µ–±—É–µ—Ç —Å–ø–µ—Ü–∏–∞–ª—å–Ω–æ–π —Å–±–æ—Ä–∫–∏ –¥–ª—è Windows.",
                "",
                "üîß –†–ï–®–ï–ù–ò–ï:",
                "1. –ó–∞–ø—É—Å—Ç–∏—Ç–µ: python install.py",
                "   –≠—Ç–æ —É—Å—Ç–∞–Ω–æ–≤–∏—Ç pre-built triton –¥–ª—è Windows",
                "",
                "2. –ò–ª–∏ –≤—Ä—É—á–Ω—É—é: pip install https://github.com/woct0rdho/triton-windows/releases/download/v3.1.0-windows.post8/triton-3.1.0-cpXXX-win_amd64.whl",
                "   (–∑–∞–º–µ–Ω–∏—Ç–µ XXX –Ω–∞ –≤–∞—à—É –≤–µ—Ä—Å–∏—é Python: 310, 311, 312)",
            ]
        elif "bitsandbytes" in error_lower:
            problem = "[ERROR] BITSANDBYTES ERROR: Could not load bitsandbytes"
            diagnosis = [
                "bitsandbytes —Ç—Ä–µ–±—É–µ—Ç CUDA –∏ —Å–ø–µ—Ü–∏–∞–ª—å–Ω–æ–π —Å–±–æ—Ä–∫–∏ –¥–ª—è Windows.",
                "",
                "üîß –†–ï–®–ï–ù–ò–ï:",
                "1. –ó–∞–ø—É—Å—Ç–∏—Ç–µ: python install.py",
                "   –≠—Ç–æ —É—Å—Ç–∞–Ω–æ–≤–∏—Ç pre-built bitsandbytes –¥–ª—è Windows",
                "",
                "2. –ò–ª–∏ –≤—Ä—É—á–Ω—É—é: pip install bitsandbytes --index-url https://jllllll.github.io/bitsandbytes-windows-webui",
            ]
        elif "torch" in error_lower or "cuda" in error_lower:
            problem = "[ERROR] TORCH/CUDA ERROR: Problem with PyTorch or CUDA"
            diagnosis = [
                "PyTorch –Ω–µ –Ω–∞—Å—Ç—Ä–æ–µ–Ω –ø—Ä–∞–≤–∏–ª—å–Ω–æ –∏–ª–∏ –æ—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç CUDA.",
                "",
                "üîß –†–ï–®–ï–ù–ò–ï:",
                "1. –ü—Ä–æ–≤–µ—Ä—å—Ç–µ —É—Å—Ç–∞–Ω–æ–≤–∫—É CUDA: nvidia-smi",
                "2. –ü–µ—Ä–µ—É—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ PyTorch —Å CUDA: pip install torch torchvision --index-url https://download.pytorch.org/whl/cu121",
            ]
        else:
            problem = f"[ERROR] IMPORT ERROR: {type(e).__name__}"
            diagnosis = [
                f"–ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –º–æ–¥—É–ª–∏ –æ–±—É—á–µ–Ω–∏—è: {e}",
                "",
                "üîß –†–ï–®–ï–ù–ò–ï:",
                "1. –ó–∞–ø—É—Å—Ç–∏—Ç–µ: python install.py",
                "2. –ü—Ä–æ–≤–µ—Ä—å—Ç–µ requirements.txt: pip install -r requirements.txt",
                "3. –£–±–µ–¥–∏—Ç–µ—Å—å, —á—Ç–æ CUDA —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∞ –ø—Ä–∞–≤–∏–ª—å–Ω–æ",
            ]
        
        # –§–æ—Ä–º–∏—Ä—É–µ–º –∫—Ä–∞—Å–∏–≤–æ–µ —Å–æ–æ–±—â–µ–Ω–∏–µ
        separator = "=" * 70
        error_lines = [
            "",
            separator,
            problem,
            separator,
            "",
        ] + diagnosis + [
            "",
            f"Python: {sys.version}",
            f"Executable: {sys.executable}",
            "",
            "–ü–æ–ª–Ω—ã–π traceback:",
            traceback_str,
            separator,
        ]
        
        error_msg = "\n".join(error_lines)
        logger.error(error_msg)
        
        # –¢–∞–∫–∂–µ –≤—ã–≤–æ–¥–∏–º –≤ –∫–æ–Ω—Å–æ–ª—å –¥–ª—è –≤–∏–¥–∏–º–æ—Å—Ç–∏
        print(error_msg)
        
        raise RuntimeError(error_msg)


class Flux2TrainModelSelect:
    """
    –í—ã–±–æ—Ä –º–æ–¥–µ–ª–µ–π Flux.2 –¥–ª—è –æ–±—É—á–µ–Ω–∏—è.
    –ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç Flux.2 Klein 9B Base –∏ Flux.2 Dev.
    """
    
    @classmethod
    def INPUT_TYPES(s):
        return {
            "required": {
                "transformer": (folder_paths.get_filename_list("unet"), {
                    "tooltip": "Flux.2 transformer model (flux2_klein_9b or flux2_dev)"
                }),
                "vae": (folder_paths.get_filename_list("vae"), {
                    "tooltip": "VAE model (ae.safetensors)"
                }),
                "clip_l": (folder_paths.get_filename_list("clip"), {
                    "tooltip": "CLIP-L text encoder"
                }),
                "t5": (folder_paths.get_filename_list("clip"), {
                    "tooltip": "T5-XXL text encoder"
                }),
            },
            "optional": {
                "lora_path": ("STRING", {
                    "multiline": True, 
                    "default": "", 
                    "tooltip": "Pre-trained LoRA path to continue training from (optional)"
                }),
            }
        }

    RETURN_TYPES = ("TRAIN_FLUX2_MODELS",)
    RETURN_NAMES = ("flux2_models",)
    FUNCTION = "loadmodel"
    CATEGORY = "FluxTrainer/Flux2"

    def loadmodel(self, transformer, vae, clip_l, t5, lora_path=""):
        # LAZY IMPORT - –∑–∞–≥—Ä—É–∂–∞–µ–º —Ç–æ–ª—å–∫–æ –ø—Ä–∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–∏
        flux_utils = _lazy_import_flux_utils()
        
        transformer_path = folder_paths.get_full_path("unet", transformer)
        vae_path = folder_paths.get_full_path("vae", vae)
        clip_path = folder_paths.get_full_path("clip", clip_l)
        t5_path = folder_paths.get_full_path("clip", t5)

        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ç–∏–ø –º–æ–¥–µ–ª–∏
        model_type = "auto"
        if flux_utils:
            try:
                is_diffusers, is_schnell, (num_double, num_single), _ = flux_utils.analyze_checkpoint_state(transformer_path)
                if num_double > 24 or num_single > 50:
                    model_type = "flux2_dev"
                    logger.info(f"Detected Flux.2 Dev model (blocks: {num_double}/{num_single})")
                else:
                    model_type = "flux2_klein_9b"
                    logger.info(f"Detected Flux.2 Klein 9B model (blocks: {num_double}/{num_single})")
            except Exception as e:
                logger.warning(f"Could not auto-detect model type: {e}")

        flux2_models = {
            "transformer": transformer_path,
            "vae": vae_path,
            "clip_l": clip_path,
            "t5": t5_path,
            "lora_path": lora_path,
            "model_type": model_type
        }
        
        return (flux2_models,)


class Flux2TrainModelPaths:
    """
    –†—É—á–Ω–æ–π –≤–≤–æ–¥ –ø—É—Ç–µ–π –∫ –º–æ–¥–µ–ª—è–º Flux.2.
    –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ, –µ—Å–ª–∏ —Ñ–∞–π–ª—ã –Ω–µ –ª–µ–∂–∞—Ç –≤ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã—Ö –ø–∞–ø–∫–∞—Ö ComfyUI.
    """

    @classmethod
    def INPUT_TYPES(s):
        return {
            "required": {
                "transformer_path": ("STRING", {
                    "default": "",
                    "multiline": False,
                    "tooltip": "Full path or filename in models/unet"
                }),
                "vae_path": ("STRING", {
                    "default": "",
                    "multiline": False,
                    "tooltip": "Full path or filename in models/vae"
                }),
                "clip_l_path": ("STRING", {
                    "default": "",
                    "multiline": False,
                    "tooltip": "Full path or filename in models/clip"
                }),
                "t5_path": ("STRING", {
                    "default": "",
                    "multiline": False,
                    "tooltip": "Full path or filename in models/clip"
                }),
            },
            "optional": {
                "lora_path": ("STRING", {
                    "multiline": True,
                    "default": "",
                    "tooltip": "Pre-trained LoRA path to continue training from"
                }),
            }
        }

    RETURN_TYPES = ("TRAIN_FLUX2_MODELS",)
    RETURN_NAMES = ("flux2_models",)
    FUNCTION = "loadmodel_paths"
    CATEGORY = "FluxTrainer/Flux2"

    def _resolve(self, path_value: str, folder_key: str, required: bool = True) -> str:
        if not path_value or not path_value.strip():
            if required:
                friendly_names = {
                    "unet": "Transformer (UNet)",
                    "vae": "VAE",
                    "clip": "CLIP / T5",
                }
                name = friendly_names.get(folder_key, folder_key)
                raise ValueError(
                    f"–ü—É—Ç—å –∫ –º–æ–¥–µ–ª–∏ '{name}' –Ω–µ —É–∫–∞–∑–∞–Ω. "
                    f"–£–∫–∞–∂–∏—Ç–µ –ø—É—Ç—å –∫ —Ñ–∞–π–ª—É –∏–ª–∏ –≤—ã–±–µ—Ä–∏—Ç–µ –º–æ–¥–µ–ª—å —á–µ—Ä–µ–∑ –≤–∏–¥–∂–µ—Ç."
                )
            return ""
        path_value = path_value.strip()
        if os.path.isabs(path_value) and os.path.exists(path_value):
            return path_value
        # Try resolve relative to ComfyUI models folders
        resolved = folder_paths.get_full_path(folder_key, path_value)
        if resolved and os.path.exists(resolved):
            return resolved
        # Fallback: direct path check (relative)
        if os.path.exists(path_value):
            return os.path.abspath(path_value)
        raise FileNotFoundError(
            f"–§–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω: '{path_value}'. "
            f"–ü—Ä–æ–≤–µ—Ä—å—Ç–µ –ø—É—Ç—å –∏ –Ω–∞–ª–∏—á–∏–µ —Ñ–∞–π–ª–∞."
        )

    def loadmodel_paths(self, transformer_path, vae_path, clip_l_path, t5_path, lora_path=""):
        # LAZY IMPORT
        flux_utils = _lazy_import_flux_utils()
        
        transformer_path = self._resolve(transformer_path, "unet")
        vae_path = self._resolve(vae_path, "vae")
        clip_path = self._resolve(clip_l_path, "clip")
        t5_path = self._resolve(t5_path, "clip")

        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ç–∏–ø –º–æ–¥–µ–ª–∏
        model_type = "auto"
        if flux_utils:
            try:
                is_diffusers, is_schnell, (num_double, num_single), _ = flux_utils.analyze_checkpoint_state(transformer_path)
                if num_double > 24 or num_single > 50:
                    model_type = "flux2_dev"
                    logger.info(f"Detected Flux.2 Dev model (blocks: {num_double}/{num_single})")
                else:
                    model_type = "flux2_klein_9b"
                    logger.info(f"Detected Flux.2 Klein 9B model (blocks: {num_double}/{num_single})")
            except Exception as e:
                logger.warning(f"Could not auto-detect model type: {e}")

        flux2_models = {
            "transformer": transformer_path,
            "vae": vae_path,
            "clip_l": clip_path,
            "t5": t5_path,
            "lora_path": lora_path,
            "model_type": model_type,
        }

        return (flux2_models,)


class Flux2LowVRAMConfig:
    """
    –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –¥–ª—è –æ–±—É—á–µ–Ω–∏—è —Å –Ω–∏–∑–∫–∏–º VRAM.
    –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –ø–æ–¥–±–∏—Ä–∞–µ—Ç –æ–ø—Ç–∏–º–∞–ª—å–Ω—ã–µ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ –¥–ª—è –≤–∞—à–µ–π GPU.
    """
    
    @classmethod
    def INPUT_TYPES(s):
        return {
            "required": {
                "strategy": (["auto", "none", "conservative", "aggressive", "extreme"], {
                    "default": "aggressive",
                    "tooltip": "Memory optimization strategy. 'auto' detects based on VRAM."
                }),
                "available_vram_gb": ("FLOAT", {
                    "default": 8.0, 
                    "min": 4.0, 
                    "max": 48.0, 
                    "step": 0.5,
                    "tooltip": "Your GPU VRAM in GB (e.g., 8.0 for RTX 3060 Ti)"
                }),
                "available_ram_gb": ("FLOAT", {
                    "default": 32.0, 
                    "min": 8.0, 
                    "max": 256.0, 
                    "step": 1.0,
                    "tooltip": "Your system RAM in GB"
                }),
                "blocks_to_swap": ("INT", {
                    "default": 20, 
                    "min": 0, 
                    "max": 50, 
                    "step": 1,
                    "tooltip": "Number of transformer blocks to swap between GPU and CPU (higher = less VRAM, slower)"
                }),
                "gradient_checkpointing": ("BOOLEAN", {
                    "default": True,
                    "tooltip": "Enable gradient checkpointing (saves memory, slightly slower)"
                }),
                "cpu_offload_checkpointing": ("BOOLEAN", {
                    "default": True,
                    "tooltip": "Offload gradient checkpoints to CPU (saves more VRAM)"
                }),
                "cache_text_encoder": ("BOOLEAN", {
                    "default": True,
                    "tooltip": "Cache text encoder outputs (recommended for low VRAM)"
                }),
                "optimizer_cpu_offload": ("BOOLEAN", {
                    "default": True,
                    "tooltip": "Keep optimizer states in RAM instead of VRAM"
                }),
            },
            "optional": {
                "use_fp8_base": ("BOOLEAN", {
                    "default": True,
                    "tooltip": "Load base model in FP8 precision (saves ~50% VRAM)"
                }),
                "empty_cache_frequently": ("BOOLEAN", {
                    "default": True,
                    "tooltip": "Aggressively clear CUDA cache (slower but more stable)"
                }),
            }
        }

    RETURN_TYPES = ("FLUX2_LOW_VRAM_CONFIG",)
    RETURN_NAMES = ("low_vram_config",)
    FUNCTION = "create_config"
    CATEGORY = "FluxTrainer/Flux2"

    def create_config(
        self, 
        strategy, 
        available_vram_gb, 
        available_ram_gb,
        blocks_to_swap,
        gradient_checkpointing,
        cpu_offload_checkpointing,
        cache_text_encoder,
        optimizer_cpu_offload,
        use_fp8_base=True,
        empty_cache_frequently=True
    ):
        # LAZY IMPORT - –∑–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥—É–ª–∏ –æ–±—É—á–µ–Ω–∏—è —Ç–æ–ª—å–∫–æ –ø—Ä–∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–∏
        modules = _lazy_import_training()
        LowVRAMConfig = modules["LowVRAMConfig"]
        OffloadStrategy = modules["OffloadStrategy"]
        get_optimal_config_for_vram = modules["get_optimal_config_for_vram"]
        
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Å—Ç—Ä–∞—Ç–µ–≥–∏—é
        if strategy == "auto":
            config = get_optimal_config_for_vram(available_vram_gb, available_ram_gb)
        else:
            strategy_enum = {
                "none": OffloadStrategy.NONE,
                "conservative": OffloadStrategy.CONSERVATIVE,
                "aggressive": OffloadStrategy.AGGRESSIVE,
                "extreme": OffloadStrategy.EXTREME
            }.get(strategy, OffloadStrategy.AGGRESSIVE)
            
            config = LowVRAMConfig(
                strategy=strategy_enum,
                available_vram_gb=available_vram_gb,
                available_ram_gb=available_ram_gb,
                blocks_to_swap=blocks_to_swap,
                gradient_checkpointing=gradient_checkpointing,
                cpu_offload_checkpointing=cpu_offload_checkpointing,
                cache_text_encoder_outputs=cache_text_encoder,
                optimizer_offload_to_cpu=optimizer_cpu_offload,
                use_fp8_base=use_fp8_base,
                empty_cache_frequently=empty_cache_frequently,
            )
        
        # –í—ã–≤–æ–¥–∏–º —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏
        mem_estimate = config.estimate_memory_usage(9.0)  # –î–ª—è Klein 9B
        logger.info(f"Low VRAM Config: strategy={config.strategy.value}")
        logger.info(f"  Blocks to swap: {config.blocks_to_swap}")
        logger.info(f"  Gradient checkpointing: {config.gradient_checkpointing}")
        logger.info(f"  Estimated VRAM usage: ~{sum(v for k,v in mem_estimate.items() if 'vram' in k):.1f}GB")
        
        return (config,)


# =============================================================================
# NODE: Flux2InitTraining - –ì–ª–∞–≤–Ω—ã–π –Ω–æ–¥ –¥–ª—è –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ –æ–±—É—á–µ–Ω–∏—è
# =============================================================================
class Flux2InitTraining:
    """
    –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –æ–±—É—á–µ–Ω–∏—è Flux.2 LoRA.
    –û—Å–Ω–æ–≤–Ω–æ–π —É–∑–µ–ª –¥–ª—è –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ —Ç—Ä–µ–Ω–∏—Ä–æ–≤–æ—á–Ω–æ–π —Å–µ—Å—Å–∏–∏.
    –û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–Ω—ã–º VRAM.
    """
    
    @classmethod
    def INPUT_TYPES(s):
        return {
            "required": {
                "flux2_models": ("TRAIN_FLUX2_MODELS",),
                "dataset": ("JSON",),
                "optimizer_settings": ("ARGS",),
                "output_name": ("STRING", {"default": "flux2_lora", "multiline": False}),
                "output_dir": ("STRING", {"default": "flux2_trainer_output", "multiline": False, 
                    "tooltip": "Output directory path (relative to ComfyUI folder)"}),
                
                # Network type - LoRA or DoRA
                "network_type": (["lora", "dora"], {
                    "default": "lora",
                    "tooltip": "LoRA - classic Low-Rank Adaptation. DoRA - Weight-Decomposed LoRA (better quality, slightly more VRAM)"
                }),
                
                # LoRA settings
                "network_dim": ("INT", {"default": 16, "min": 1, "max": 128, "step": 1,
                    "tooltip": "LoRA rank (dim). Lower = less VRAM. Recommended: 8-32 for low VRAM"}),
                "network_alpha": ("FLOAT", {"default": 16.0, "min": 0.1, "max": 128.0, "step": 0.1}),
                
                # Training settings
                "learning_rate": ("FLOAT", {"default": 1e-4, "min": 1e-8, "max": 1.0, "step": 1e-6,
                    "tooltip": "Learning rate. Recommended: 1e-4 to 5e-4"}),
                "max_train_steps": ("INT", {"default": 1000, "min": 1, "max": 100000, "step": 1}),
                
                # Data settings
                "cache_latents": (["disk", "memory", "disabled"], {"default": "disk",
                    "tooltip": "Cache VAE latents. 'disk' recommended for low VRAM"}),
                "cache_text_encoder_outputs": (["disk", "memory", "disabled"], {"default": "disk",
                    "tooltip": "Cache text encoder outputs. 'disk' recommended for low VRAM"}),
                
                # Precision
                "gradient_dtype": (["bf16", "fp16"], {"default": "bf16"}),
                "save_dtype": (["bf16", "fp16", "fp32"], {"default": "bf16"}),
                
                # Memory optimization
                "optimizer_fusing": (["fused_backward_pass", "blockwise_fused_optimizers"], {
                    "default": "fused_backward_pass",
                    "tooltip": "Memory optimization for optimizer. Both significantly reduce VRAM"}),
                
                # Sample prompts
                "sample_prompts": ("STRING", {"multiline": True, 
                    "default": "a photo of sks person | a painting of sks person in anime style",
                    "tooltip": "Sample prompts for validation. Separate multiple prompts with |"}),
            },
            "optional": {
                "low_vram_config": ("FLUX2_LOW_VRAM_CONFIG", {
                    "tooltip": "Low VRAM configuration from Flux2LowVRAMConfig node"
                }),
                "weighting_scheme": (["logit_normal", "sigma_sqrt", "mode", "cosmap", "none"], {
                    "default": "logit_normal",
                    "tooltip": "Timestep weighting scheme. logit_normal recommended for Flux"
                }),
                "timestep_sampling": (["sigmoid", "uniform", "shift"], {
                    "default": "sigmoid",
                    "tooltip": "Timestep sampling method. sigmoid recommended for Flux"
                }),
                "auto_resume": ("BOOLEAN", {
                    "default": True,
                    "tooltip": "Automatically resume from latest checkpoint if found in output_dir"
                }),
                "check_vram": ("BOOLEAN", {
                    "default": True,
                    "tooltip": "Check VRAM availability before training and warn if insufficient"
                }),
                "additional_args": ("STRING", {"multiline": True, "default": "",
                    "tooltip": "Additional training arguments"}),
                "seed": ("INT", {"default": 42, "min": 0, "max": 2**32-1}),
            },
            "hidden": {
                "prompt": "PROMPT", 
                "extra_pnginfo": "EXTRA_PNGINFO"
            },
        }

    RETURN_TYPES = ("NETWORKTRAINER", "INT", "STRING", "KOHYA_ARGS")
    RETURN_NAMES = ("network_trainer", "epochs_count", "output_path", "args")
    FUNCTION = "init_training"
    CATEGORY = "FluxTrainer/Flux2"

    def init_training(
        self, 
        flux2_models, 
        dataset, 
        optimizer_settings, 
        output_name,
        output_dir,
        network_type,
        network_dim, 
        network_alpha,
        learning_rate,
        max_train_steps,
        cache_latents,
        cache_text_encoder_outputs,
        gradient_dtype,
        save_dtype,
        optimizer_fusing,
        sample_prompts,
        low_vram_config=None,
        weighting_scheme="logit_normal",
        timestep_sampling="sigmoid",
        auto_resume=True,
        check_vram=True,
        additional_args=None,
        seed=42,
        prompt=None, 
        extra_pnginfo=None,
    ):
        # LAZY IMPORT - –∫–ª—é—á–µ–≤–æ–µ –∏–∑–º–µ–Ω–µ–Ω–∏–µ –¥–ª—è —Å—Ç–∞–±–∏–ª—å–Ω–æ–π –∑–∞–≥—Ä—É–∑–∫–∏!
        # –û—à–∏–±–∫–∞ –ø–æ—è–≤–∏—Ç—Å—è –¢–û–õ–¨–ö–û –∑–¥–µ—Å—å –ø—Ä–∏ Queue Prompt, –∞ –Ω–µ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ –Ω–æ–¥
        modules = _lazy_import_training()
        toml = modules["toml"]
        torch = modules["torch"]
        FluxNetworkTrainer = modules["FluxNetworkTrainer"]
        train_network_setup_parser = modules["train_network_setup_parser"]
        flux_train_utils = modules["flux_train_utils"]
        get_optimal_config_for_vram = modules["get_optimal_config_for_vram"]
        estimate_vram_usage = modules["estimate_vram_usage"]
        print_vram_estimate = modules["print_vram_estimate"]
        auto_resume_fn = modules["auto_resume_training"]
        get_training_progress = modules["get_training_progress"]
        find_latest_checkpoint = modules["find_latest_checkpoint"]
        
        mm.soft_empty_cache()
        
        # ===================================================================
        # VALIDATION - –ü—Ä–æ–≤–µ—Ä—è–µ–º –ø–∞—Ä–∞–º–µ—Ç—Ä—ã LoRA –¥–ª—è –ø—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–µ–Ω–∏—è "garbage LoRA"
        # ===================================================================
        # –ü—Ä–∞–≤–∏–ª–æ: network_alpha –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å <= network_dim
        # –ï—Å–ª–∏ alpha > dim, –≤–µ—Å–∞ "–≤–∑—Ä—ã–≤–∞—é—Ç—Å—è" –∏ LoRA –ø–æ–ª—É—á–∞–µ—Ç—Å—è –±–∏—Ç–æ–π
        if network_alpha > network_dim:
            logger.warning(
                f"[WARN] network_alpha ({network_alpha}) > network_dim ({network_dim})! "
                f"–≠—Ç–æ –º–æ–∂–µ—Ç –ø—Ä–∏–≤–µ—Å—Ç–∏ –∫ –Ω–µ—Å—Ç–∞–±–∏–ª—å–Ω–æ–º—É –æ–±—É—á–µ–Ω–∏—é. "
                f"–ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ —É—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞—é network_alpha = {network_dim}"
            )
            network_alpha = float(network_dim)
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ network_type
        is_dora = network_type.lower() == "dora"
        
        # ===================================================================
        # SAFE MODE - Fallback –Ω–∞ Adafactor –µ—Å–ª–∏ bitsandbytes –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω
        # ===================================================================
        bnb_available = False
        try:
            import bitsandbytes
            bnb_available = True
        except ImportError:
            logger.warning("[WARN] bitsandbytes not available. 8-bit optimizers disabled.")
        
        # –ü–æ–ª—É—á–∞–µ–º —Ç–∏–ø –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä–∞ –∏–∑ –Ω–∞—Å—Ç—Ä–æ–µ–∫
        current_optimizer = optimizer_settings.get("optimizer_type", "adafactor")
        
        # –°–ø–∏—Å–æ–∫ –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä–æ–≤, —Ç—Ä–µ–±—É—é—â–∏—Ö bitsandbytes
        bnb_optimizers = ["adamw8bit", "lion8bit", "ademamix8bit", "pagedademamix8bit"]
        
        if current_optimizer.lower() in [o.lower() for o in bnb_optimizers] and not bnb_available:
            logger.warning(
                f"[WARN] Optimizer '{current_optimizer}' requires bitsandbytes which is not available. "
                f"Automatically switching to Adafactor (works without bitsandbytes)."
            )
            optimizer_settings["optimizer_type"] = "adafactor"
            optimizer_settings["optimizer_args"] = [
                "scale_parameter=False",
                "relative_step=False",
                "warmup_init=False"
            ]
        
        # –°–æ–∑–¥–∞—ë–º –∫–æ–Ω—Ñ–∏–≥ –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é –µ—Å–ª–∏ –Ω–µ –ø–µ—Ä–µ–¥–∞–Ω
        if low_vram_config is None:
            low_vram_config = get_optimal_config_for_vram(8.0, 32.0)
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é
        output_dir = os.path.abspath(output_dir)
        os.makedirs(output_dir, exist_ok=True)
        
        total, used, free = shutil.disk_usage(output_dir)
        required_free_space = 2 * (2**30)  # 2 GB –º–∏–Ω–∏–º—É–º
        if free <= required_free_space:
            raise ValueError(f"Insufficient disk space. Required: {required_free_space/2**30:.1f}GB. Available: {free/2**30:.1f}GB")
        
        # ===================================================================
        # VRAM SAFETY CHECK - –ü—Ä–æ–≤–µ—Ä—è–µ–º –¥–æ—Å—Ç—É–ø–Ω—É—é –ø–∞–º—è—Ç—å –ø–µ—Ä–µ–¥ –æ–±—É—á–µ–Ω–∏–µ–º
        # ===================================================================
        if check_vram:
            try:
                # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ä–∞–∑–º–µ—Ä –º–æ–¥–µ–ª–∏ –∏–∑ model_type
                model_type_str = flux2_models.get("model_type", "auto")
                if model_type_str in ("flux2_dev", "flux_12b"):
                    model_params_b = 12.0
                else:
                    model_params_b = 9.0  # Klein 9B –∏–ª–∏ auto
                
                import torch
                gpu_vram_gb = 8.0
                if torch.cuda.is_available():
                    gpu_vram_gb = torch.cuda.get_device_properties(0).total_memory / (1024 ** 3)
                
                vram_estimate = estimate_vram_usage(
                    model_params_billions=model_params_b,
                    network_dim=network_dim,
                    batch_size=1,
                    use_fp8_base=low_vram_config.use_fp8_base,
                    gradient_checkpointing=low_vram_config.gradient_checkpointing,
                    cache_text_encoder=cache_text_encoder_outputs != "disabled",
                    blocks_to_swap=low_vram_config.blocks_to_swap,
                    available_vram_gb=gpu_vram_gb,
                )
                print_vram_estimate(vram_estimate)
                
                if vram_estimate.risk_level == "critical":
                    raise ValueError(
                        f"VRAM CRITICAL: –¢—Ä–µ–±—É–µ—Ç—Å—è ~{vram_estimate.total_estimated_gb:.1f}GB, "
                        f"–¥–æ—Å—Ç—É–ø–Ω–æ {vram_estimate.available_vram_gb:.1f}GB. "
                        "–£–º–µ–Ω—å—à–∏—Ç–µ network_dim –∏–ª–∏ –≤–∫–ª—é—á–∏—Ç–µ FP8 base."
                    )
                elif vram_estimate.risk_level == "danger":
                    logger.warning(
                        f"[WARN] VRAM WARNING: Required ~{vram_estimate.total_estimated_gb:.1f}GB, "
                        f"–¥–æ—Å—Ç—É–ø–Ω–æ {vram_estimate.available_vram_gb:.1f}GB. –í–æ–∑–º–æ–∂–Ω—ã –ø—Ä–æ–±–ª–µ–º—ã —Å –ø–∞–º—è—Ç—å—é."
                    )
            except Exception as e:
                logger.warning(f"Could not estimate VRAM: {e}")
        
        # ===================================================================
        # AUTO-RESUME - –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ –ø—Ä–æ–¥–æ–ª–∂–µ–Ω–∏–µ —Å –ø–æ—Å–ª–µ–¥–Ω–µ–≥–æ —á–µ–∫–ø–æ–∏–Ω—Ç–∞
        # ===================================================================
        resume_checkpoint = None
        if auto_resume:
            try:
                latest_ckpt = find_latest_checkpoint(output_dir)
                if latest_ckpt:
                    resume_checkpoint = latest_ckpt
                    progress = get_training_progress(output_dir)
                    logger.info("=" * 60)
                    logger.info("[AUTO-RESUME] Found checkpoint to continue!")
                    logger.info(f"   –§–∞–π–ª: {os.path.basename(resume_checkpoint)}")
                    if progress:
                        logger.info(f"   –ü—Ä–æ–≥—Ä–µ—Å—Å: —à–∞–≥ {progress.get('last_step', '?')}, "
                                  f"—ç–ø–æ—Ö–∞ {progress.get('last_epoch', '?')}")
                    logger.info("=" * 60)
                else:
                    logger.info("[AUTO-RESUME] No previous checkpoint found. Starting fresh.")
            except Exception as e:
                logger.warning(f"Auto-resume check failed: {e}")
        
        # –ü–∞—Ä—Å–∏–º –¥–∞—Ç–∞—Å–µ—Ç
        dataset_config = dataset["datasets"]
        dataset_json = json.loads(dataset_config)
        
        # –ü–æ–ª—É—á–∞–µ–º —Ä–∞–∑–º–µ—Ä—ã –∏–∑ –¥–∞—Ç–∞—Å–µ—Ç–∞  
        width = dataset.get("width", 1024)
        height = dataset.get("height", 1024)
        
        # –ù–∞—Å—Ç—Ä–∞–∏–≤–∞–µ–º –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ
        cache_latents_to_disk = cache_latents == "disk"
        cache_latents_enabled = cache_latents != "disabled"
        cache_te_to_disk = cache_text_encoder_outputs == "disk"
        cache_te_enabled = cache_text_encoder_outputs != "disabled"
        
        # ========================================================
        # AUTO-FIX: –ü—Ä–∏ –≤–∫–ª—é—á—ë–Ω–Ω–æ–º –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏–∏ Text Encoder
        # –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –æ—Ç–∫–ª—é—á–∞–µ–º –Ω–µ—Å–æ–≤–º–µ—Å—Ç–∏–º—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –¥–∞—Ç–∞—Å–µ—Ç–∞
        # (shuffle_caption, caption_dropout_rate, token_warmup_step,
        #  caption_tag_dropout_rate)
        # ========================================================
        if cache_te_enabled and "general" in dataset_json:
            general = dataset_json["general"]
            incompatible_keys = {
                "shuffle_caption": False,
                "caption_dropout_rate": 0.0,
                "token_warmup_step": 0,
                "caption_tag_dropout_rate": 0.0,
            }
            fixed_keys = []
            for key, safe_value in incompatible_keys.items():
                if key in general:
                    current = general[key]
                    # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–æ –ª–∏ –Ω–µ—Å–æ–≤–º–µ—Å—Ç–∏–º–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ
                    if isinstance(current, bool) and current:
                        general[key] = safe_value
                        fixed_keys.append(f"{key}: {current} ‚Üí {safe_value}")
                    elif isinstance(current, (int, float)) and current > 0:
                        general[key] = safe_value
                        fixed_keys.append(f"{key}: {current} ‚Üí {safe_value}")
            if fixed_keys:
                logger.warning(
                    "[AUTO-FIX] –ö—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ Text Encoder –≤–∫–ª—é—á–µ–Ω–æ ‚Äî "
                    "–∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –æ—Ç–∫–ª—é—á–µ–Ω—ã –Ω–µ—Å–æ–≤–º–µ—Å—Ç–∏–º—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –¥–∞—Ç–∞—Å–µ—Ç–∞:\n  "
                    + "\n  ".join(fixed_keys)
                )
        
        dataset_toml = toml.dumps(dataset_json)
        
        # –°–æ–∑–¥–∞—ë–º –ø–∞—Ä—Å–µ—Ä –∏ –∞—Ä–≥—É–º–µ–Ω—Ç—ã
        parser = train_network_setup_parser()
        flux_train_utils.add_flux_train_arguments(parser)
        
        if additional_args:
            args, _ = parser.parse_known_args(args=shlex.split(additional_args))
        else:
            args, _ = parser.parse_known_args()
        
        # –ü–∞—Ä—Å–∏–º sample prompts
        if '|' in sample_prompts:
            prompts_list = [p.strip() for p in sample_prompts.split('|')]
        else:
            prompts_list = [sample_prompts.strip()]
        
        # –§–æ—Ä–º–∏—Ä—É–µ–º –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é
        network_suffix = "dora" if is_dora else "lora"
        
        # Network args –¥–ª—è LoRA/DoRA –∏ Flux-specific –Ω–∞—Å—Ç—Ä–æ–π–∫–∏
        network_args_dict = {}
        if is_dora:
            # DoRA: Weight-Decomposed Low-Rank Adaptation
            # –î–æ–±–∞–≤–ª—è–µ—Ç decomposed weight magnitude –¥–ª—è –ª—É—á—à–µ–≥–æ –∫–∞—á–µ—Å—Ç–≤–∞
            network_args_dict["dora_wd"] = True
        
        # Flux-specific: train_on_input —É–ª—É—á—à–∞–µ—Ç –∫–∞—á–µ—Å—Ç–≤–æ –Ω–∞ –Ω–µ–∫–æ—Ç–æ—Ä—ã—Ö –º–æ–¥–µ–ª—è—Ö
        # –û–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ –º–æ–∂–Ω–æ –¥–æ–±–∞–≤–∏—Ç—å —á–µ—Ä–µ–∑ additional_args
        
        config_dict = {
            # –ú–æ–¥–µ–ª–∏
            "pretrained_model_name_or_path": flux2_models["transformer"],
            "clip_l": flux2_models["clip_l"],
            "t5xxl": flux2_models["t5"],
            "ae": flux2_models["vae"],
            
            # LoRA/DoRA
            "network_module": ".networks.lora_flux",
            "network_dim": network_dim,
            "network_alpha": network_alpha,
            "network_args": network_args_dict if network_args_dict else None,
            
            # Training
            "learning_rate": learning_rate,
            "max_train_steps": max_train_steps,
            "seed": seed,
            
            # Output - –≤–∫–ª—é—á–∞–µ–º —Ç–∏–ø —Å–µ—Ç–∏ –≤ –∏–º—è
            "output_dir": output_dir,
            "output_name": f"{output_name}_{network_suffix}_rank{network_dim}_{save_dtype}",
            "save_model_as": "safetensors",
            "save_precision": save_dtype,
            
            # Dataset
            "dataset_config": dataset_toml,
            "width": int(width),
            "height": int(height),
            
            # Caching
            "cache_latents": cache_latents_enabled,
            "cache_latents_to_disk": cache_latents_to_disk,
            "cache_text_encoder_outputs": cache_te_enabled,
            "cache_text_encoder_outputs_to_disk": cache_te_to_disk,
            
            # Precision
            "mixed_precision": gradient_dtype,
            "full_bf16": gradient_dtype == "bf16",
            "full_fp16": gradient_dtype == "fp16",
            
            # Memory optimizations from low_vram_config
            "gradient_checkpointing": low_vram_config.gradient_checkpointing,
            "cpu_offload_checkpointing": low_vram_config.cpu_offload_checkpointing,
            "blocks_to_swap": low_vram_config.blocks_to_swap,
            "fp8_base": low_vram_config.use_fp8_base,
            "fp8_base_unet": low_vram_config.use_fp8_base,
            
            # Optimizer fusing
            "fused_backward_pass": optimizer_fusing == "fused_backward_pass",
            "blockwise_fused_optimizers": optimizer_fusing == "blockwise_fused_optimizers",
            
            # Misc
            "sample_prompts": prompts_list,
            "network_train_unet_only": True,
            "persistent_data_loader_workers": False,
            "max_data_loader_n_workers": 2,
            "num_cpu_threads_per_process": 1,
            "disable_mmap_load_safetensors": False,
            "mem_eff_attn": True,
            "xformers": False,
            "sdpa": True,
            
            # Flux-specific - –∏—Å–ø–æ–ª—å–∑—É–µ–º –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –∏–∑ INPUT
            "t5xxl_max_token_length": 512,
            "apply_t5_attn_mask": True,
            "weighting_scheme": weighting_scheme,
            "logit_mean": 0.0,
            "logit_std": 1.0,
            "mode_scale": 1.29,
            "guidance_scale": 1.0,
            "discrete_flow_shift": 1.0,
            "loss_type": "l2",
            "timestep_sampling": timestep_sampling,
            "sigmoid_scale": 1.0,
            "model_prediction_type": "raw",
            "alpha_mask": dataset.get("alpha_mask", False),
        }
        
        # –î–æ–±–∞–≤–ª—è–µ–º lora_path –µ—Å–ª–∏ –µ—Å—Ç—å (–¥–ª—è fine-tuning —Å—É—â–µ—Å—Ç–≤—É—é—â–µ–π LoRA)
        if flux2_models.get("lora_path"):
            config_dict["network_weights"] = flux2_models["lora_path"]
        
        # –î–æ–±–∞–≤–ª—è–µ–º resume checkpoint –µ—Å–ª–∏ –Ω–∞–π–¥–µ–Ω
        if resume_checkpoint:
            config_dict["network_weights"] = resume_checkpoint
            logger.info(f"[RESUME] Resuming from: {os.path.basename(resume_checkpoint)}")
        
        # –û–±–Ω–æ–≤–ª—è–µ–º –∏–∑ optimizer_settings
        config_dict.update(optimizer_settings)
        
        # –ü—Ä–∏–º–µ–Ω—è–µ–º –∫ args
        for key, value in config_dict.items():
            setattr(args, key, value)
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é
        saved_args_file_path = os.path.join(output_dir, f"{output_name}_args.json")
        with open(saved_args_file_path, 'w', encoding='utf-8') as f:
            json.dump(vars(args), f, indent=4, ensure_ascii=False)
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º workflow
        if extra_pnginfo is not None:
            saved_workflow_file_path = os.path.join(output_dir, f"{output_name}_workflow.json")
            with open(saved_workflow_file_path, 'w', encoding='utf-8') as f:
                json.dump(extra_pnginfo.get("workflow", {}), f, indent=4, ensure_ascii=False)
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º —Ç—Ä–µ–Ω–µ—Ä
        logger.info("=" * 60)
        logger.info(f"Initializing Flux.2 {'DoRA' if is_dora else 'LoRA'} Training")
        logger.info(f"  Model type: {flux2_models.get('model_type', 'unknown')}")
        logger.info(f"  Network type: {network_type.upper()}")
        logger.info(f"  Output: {output_dir}/{output_name}")
        logger.info(f"  Network dim: {network_dim}, alpha: {network_alpha}")
        logger.info(f"  Blocks to swap: {low_vram_config.blocks_to_swap}")
        logger.info(f"  FP8 base: {low_vram_config.use_fp8_base}")
        if resume_checkpoint:
            logger.info(f"  Resuming from: {os.path.basename(resume_checkpoint)}")
        logger.info("=" * 60)
        
        with torch.inference_mode(False):
            network_trainer = FluxNetworkTrainer()
            training_loop = network_trainer.init_train(args)
        
        final_output_path = os.path.join(output_dir, f"{output_name}_rank{network_dim}_{save_dtype}")
        epochs_count = network_trainer.num_train_epochs
        
        trainer = {
            "network_trainer": network_trainer,
            "training_loop": training_loop,
        }
        
        return (trainer, epochs_count, final_output_path, args)


# =============================================================================
# NODE: Flux2TrainLoop - –¶–∏–∫–ª –æ–±—É—á–µ–Ω–∏—è
# =============================================================================
class Flux2TrainLoop:
    """
    –¶–∏–∫–ª –æ–±—É—á–µ–Ω–∏—è Flux.2 LoRA.
    –í—ã–ø–æ–ª–Ω—è–µ—Ç —É–∫–∞–∑–∞–Ω–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —à–∞–≥–æ–≤ –æ–±—É—á–µ–Ω–∏—è.
    """
    
    @classmethod
    def INPUT_TYPES(s):
        return {
            "required": {
                "network_trainer": ("NETWORKTRAINER",),
                "steps": ("INT", {"default": 100, "min": 1, "max": 10000, "step": 1,
                    "tooltip": "Number of training steps to run"}),
            },
        }

    RETURN_TYPES = ("NETWORKTRAINER", "INT",)
    RETURN_NAMES = ("network_trainer", "current_step",)
    FUNCTION = "train"
    CATEGORY = "FluxTrainer/Flux2"

    def train(self, network_trainer, steps):
        # LAZY IMPORT
        modules = _lazy_import_training()
        torch = modules["torch"]
        
        with torch.inference_mode(False):
            training_loop = network_trainer["training_loop"]
            trainer = network_trainer["network_trainer"]
            
            target_global_step = trainer.global_step + steps
            comfy_pbar = comfy.utils.ProgressBar(steps)
            trainer.comfy_pbar = comfy_pbar
            
            trainer.optimizer_train_fn()
            
            while trainer.global_step < target_global_step:
                steps_done = training_loop(
                    break_at_steps=target_global_step,
                    epoch=trainer.current_epoch.value,
                )
                
                # –ü—Ä–µ—Ä—ã–≤–∞–µ–º –µ—Å–ª–∏ –¥–æ—Å—Ç–∏–≥–ª–∏ –º–∞–∫—Å–∏–º—É–º–∞
                if trainer.global_step >= trainer.args.max_train_steps:
                    break
            
            result = {
                "network_trainer": trainer,
                "training_loop": training_loop,
            }
        
        return (result, trainer.global_step)


# =============================================================================
# NODE: Flux2TrainAndValidateLoop - –û–±—É—á–µ–Ω–∏–µ —Å –≤–∞–ª–∏–¥–∞—Ü–∏–µ–π
# =============================================================================
class Flux2TrainAndValidateLoop:
    """
    –¶–∏–∫–ª –æ–±—É—á–µ–Ω–∏—è —Å –ø–µ—Ä–∏–æ–¥–∏—á–µ—Å–∫–æ–π –≤–∞–ª–∏–¥–∞—Ü–∏–µ–π –∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ–º.
    –†–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è –¥–ª—è –¥–ª–∏—Ç–µ–ª—å–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è.
    """
    
    @classmethod
    def INPUT_TYPES(cls):
        return {
            "required": {
                "network_trainer": ("NETWORKTRAINER",),
                "validate_at_steps": ("INT", {"default": 250, "min": 1, "max": 10000, "step": 1,
                    "tooltip": "Generate validation samples every N steps"}),
                "save_at_steps": ("INT", {"default": 500, "min": 1, "max": 10000, "step": 1,
                    "tooltip": "Save checkpoint every N steps"}),
            },
            "optional": {
                "validation_settings": ("VALSETTINGS",),
            }
        }

    RETURN_TYPES = ("NETWORKTRAINER", "INT",)
    RETURN_NAMES = ("network_trainer", "final_step",)
    FUNCTION = "train"
    CATEGORY = "FluxTrainer/Flux2"

    def train(self, network_trainer, validate_at_steps, save_at_steps, validation_settings=None):
        # LAZY IMPORT
        modules = _lazy_import_training()
        torch = modules["torch"]
        
        with torch.inference_mode(False):
            training_loop = network_trainer["training_loop"]
            trainer = network_trainer["network_trainer"]
            
            target_global_step = trainer.args.max_train_steps
            comfy_pbar = comfy.utils.ProgressBar(target_global_step)
            trainer.comfy_pbar = comfy_pbar
            
            trainer.optimizer_train_fn()
            
            while trainer.global_step < target_global_step:
                next_validate_step = ((trainer.global_step // validate_at_steps) + 1) * validate_at_steps
                next_save_step = ((trainer.global_step // save_at_steps) + 1) * save_at_steps
                
                steps_done = training_loop(
                    break_at_steps=min(next_validate_step, next_save_step),
                    epoch=trainer.current_epoch.value,
                )
                
                # –í–∞–ª–∏–¥–∞—Ü–∏—è
                if trainer.global_step % validate_at_steps == 0:
                    self._validate(trainer, validation_settings)
                
                # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ
                if trainer.global_step % save_at_steps == 0:
                    self._save(trainer)
                
                # –ü—Ä–µ—Ä—ã–≤–∞–µ–º –µ—Å–ª–∏ –¥–æ—Å—Ç–∏–≥–ª–∏ –º–∞–∫—Å–∏–º—É–º–∞
                if trainer.global_step >= trainer.args.max_train_steps:
                    break
            
            result = {
                "network_trainer": trainer,
                "training_loop": training_loop,
            }
        
        return (result, trainer.global_step)
    
    def _validate(self, trainer, validation_settings=None):
        params = (
            trainer.current_epoch.value,
            trainer.global_step,
            validation_settings
        )
        trainer.optimizer_eval_fn()
        image_tensors = trainer.sample_images(*params)
        trainer.optimizer_train_fn()
        logger.info(f"Validation at step: {trainer.global_step}")
    
    def _save(self, trainer):
        # LAZY IMPORT
        modules = _lazy_import_training()
        train_util = modules["train_util"]
        
        ckpt_name = train_util.get_step_ckpt_name(
            trainer.args, 
            "." + trainer.args.save_model_as, 
            trainer.global_step
        )
        trainer.optimizer_eval_fn()
        trainer.save_model(
            ckpt_name, 
            trainer.accelerator.unwrap_model(trainer.network), 
            trainer.global_step, 
            trainer.current_epoch.value + 1
        )
        trainer.optimizer_train_fn()
        logger.info(f"Saved checkpoint at step: {trainer.global_step}")


# =============================================================================
# NODE: Flux2TrainSave - –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ LoRA
# =============================================================================
class Flux2TrainSave:
    """
    –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –æ–±—É—á–µ–Ω–Ω–æ–π LoRA –º–æ–¥–µ–ª–∏.
    """
    
    @classmethod
    def INPUT_TYPES(s):
        return {
            "required": {
                "network_trainer": ("NETWORKTRAINER",),
                "save_state": ("BOOLEAN", {"default": False,
                    "tooltip": "Also save the full training state (for resume)"}),
                "copy_to_comfy_lora_folder": ("BOOLEAN", {"default": True,
                    "tooltip": "Copy LoRA to ComfyUI loras folder"}),
            },
        }

    RETURN_TYPES = ("NETWORKTRAINER", "STRING", "INT",)
    RETURN_NAMES = ("network_trainer", "lora_path", "steps",)
    FUNCTION = "save"
    CATEGORY = "FluxTrainer/Flux2"

    def save(self, network_trainer, save_state, copy_to_comfy_lora_folder):
        # LAZY IMPORT
        modules = _lazy_import_training()
        torch = modules["torch"]
        train_util = modules["train_util"]
        
        with torch.inference_mode(False):
            trainer = network_trainer["network_trainer"]
            global_step = trainer.global_step
            
            ckpt_name = train_util.get_step_ckpt_name(
                trainer.args, 
                "." + trainer.args.save_model_as, 
                global_step
            )
            trainer.save_model(
                ckpt_name, 
                trainer.accelerator.unwrap_model(trainer.network), 
                global_step, 
                trainer.current_epoch.value + 1
            )
            
            # –£–¥–∞–ª—è–µ–º —Å—Ç–∞—Ä—ã–µ —á–µ–∫–ø–æ–∏–Ω—Ç—ã
            remove_step_no = train_util.get_remove_step_no(trainer.args, global_step)
            if remove_step_no is not None:
                remove_ckpt_name = train_util.get_step_ckpt_name(
                    trainer.args, 
                    "." + trainer.args.save_model_as, 
                    remove_step_no
                )
                trainer.remove_model(remove_ckpt_name)
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Å–æ—Å—Ç–æ—è–Ω–∏–µ –µ—Å–ª–∏ –Ω—É–∂–Ω–æ
            if save_state:
                train_util.save_and_remove_state_stepwise(trainer.args, trainer.accelerator, global_step)
            
            lora_path = os.path.join(trainer.args.output_dir, ckpt_name)
            
            # –ö–æ–ø–∏—Ä—É–µ–º –≤ –ø–∞–ø–∫—É loras
            if copy_to_comfy_lora_folder:
                destination_dir = os.path.join(folder_paths.models_dir, "loras", "flux2_trainer")
                os.makedirs(destination_dir, exist_ok=True)
                shutil.copy(lora_path, os.path.join(destination_dir, ckpt_name))
                logger.info(f"Copied LoRA to: {destination_dir}/{ckpt_name}")
        
        return (network_trainer, lora_path, global_step)


# =============================================================================
# NODE: Flux2TrainEnd - –ó–∞–≤–µ—Ä—à–µ–Ω–∏–µ –æ–±—É—á–µ–Ω–∏—è
# =============================================================================
class Flux2TrainEnd:
    """
    –ó–∞–≤–µ—Ä—à–µ–Ω–∏–µ –æ–±—É—á–µ–Ω–∏—è –∏ –æ—á–∏—Å—Ç–∫–∞ —Ä–µ—Å—É—Ä—Å–æ–≤.
    """
    
    @classmethod
    def INPUT_TYPES(s):
        return {
            "required": {
                "network_trainer": ("NETWORKTRAINER",),
            },
        }

    RETURN_TYPES = ("STRING",)
    RETURN_NAMES = ("final_lora_path",)
    FUNCTION = "end_training"
    CATEGORY = "FluxTrainer/Flux2"

    def end_training(self, network_trainer):
        # LAZY IMPORT
        modules = _lazy_import_training()
        train_util = modules["train_util"]
        clean_memory_on_device = modules["clean_memory_on_device"]
        
        trainer = network_trainer["network_trainer"]
        
        # –§–∏–Ω–∞–ª—å–Ω–æ–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ
        final_ckpt_name = train_util.get_last_ckpt_name(
            trainer.args, 
            "." + trainer.args.save_model_as
        )
        trainer.save_model(
            final_ckpt_name,
            trainer.accelerator.unwrap_model(trainer.network),
            trainer.global_step,
            trainer.current_epoch.value + 1
        )
        
        final_path = os.path.join(trainer.args.output_dir, final_ckpt_name)
        
        # –û—á–∏—Å—Ç–∫–∞
        trainer.accelerator.end_training()
        clean_memory_on_device(trainer.accelerator.device)
        mm.soft_empty_cache()
        
        logger.info("=" * 60)
        logger.info("Training completed!")
        logger.info(f"  Final LoRA saved to: {final_path}")
        logger.info(f"  Total steps: {trainer.global_step}")
        logger.info("=" * 60)
        
        # === FluxTrainer Pro Dashboard: mark training finished ===
        try:
            from .training_state import TrainingState
            TrainingState.instance().finish_training(
                success=True,
                message=f"LoRA saved: {final_path}, steps: {trainer.global_step}"
            )
        except Exception:
            pass

        return (final_path,)


# =============================================================================
# NODE: Flux2TrainAdvancedSettings - –†–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–µ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏
# =============================================================================
class Flux2TrainAdvancedSettings:
    """
    –†–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–µ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ –æ–±—É—á–µ–Ω–∏—è Flux.2.
    –î–ª—è –æ–ø—ã—Ç–Ω—ã—Ö –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π.
    """
    
    @classmethod
    def INPUT_TYPES(s):
        return {
            "required": {
                # Precision settings
                "mixed_precision": (["bf16", "fp16", "no"], {"default": "bf16"}),
                "full_bf16": ("BOOLEAN", {"default": False}),
                "fp8_base": ("BOOLEAN", {"default": True}),
                
                # Training dynamics
                "timestep_sampling": (["sigmoid", "uniform", "logit_normal", "sigma", "shift", "flux_shift"], 
                    {"default": "sigmoid"}),
                "sigmoid_scale": ("FLOAT", {"default": 1.0, "min": 0.1, "max": 10.0, "step": 0.1}),
                "model_prediction_type": (["raw", "additive", "sigma_scaled"], {"default": "raw"}),
                "discrete_flow_shift": ("FLOAT", {"default": 1.0, "min": 0.0, "max": 10.0, "step": 0.1}),
                "guidance_scale": ("FLOAT", {"default": 1.0, "min": 1.0, "max": 20.0, "step": 0.5}),
                
                # Weighting
                "weighting_scheme": (["logit_normal", "sigma_sqrt", "mode", "cosmap", "none"], 
                    {"default": "logit_normal"}),
                "logit_mean": ("FLOAT", {"default": 0.0, "min": -1.0, "max": 1.0, "step": 0.01}),
                "logit_std": ("FLOAT", {"default": 1.0, "min": 0.1, "max": 2.0, "step": 0.01}),
                
                # Regularization
                "network_dropout": ("FLOAT", {"default": 0.0, "min": 0.0, "max": 0.5, "step": 0.01}),
                "scale_weight_norms": ("FLOAT", {"default": 0.0, "min": 0.0, "max": 10.0, "step": 0.1}),
                
                # Memory
                "max_data_loader_n_workers": ("INT", {"default": 2, "min": 0, "max": 16}),
                "persistent_data_loader_workers": ("BOOLEAN", {"default": False}),
            },
            "optional": {
                "t5xxl_max_token_length": ("INT", {"default": 512, "min": 77, "max": 1024}),
                "apply_t5_attn_mask": ("BOOLEAN", {"default": True}),
            }
        }

    RETURN_TYPES = ("FLUX2_ADVANCED_SETTINGS",)
    RETURN_NAMES = ("advanced_settings",)
    FUNCTION = "create_settings"
    CATEGORY = "FluxTrainer/Flux2"

    def create_settings(self, **kwargs):
        return (kwargs,)


# =============================================================================
# NODE: Flux2MemoryEstimator - –û—Ü–µ–Ω–∫–∞ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –ø–∞–º—è—Ç–∏
# =============================================================================
class Flux2MemoryEstimator:
    """
    –û—Ü–µ–Ω–∏–≤–∞–µ—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –ø–∞–º—è—Ç–∏ –¥–ª—è –≤—ã–±—Ä–∞–Ω–Ω–æ–π –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏.
    –ü–æ–º–æ–≥–∞–µ—Ç –æ–ø—Ä–µ–¥–µ–ª–∏—Ç—å –æ–ø—Ç–∏–º–∞–ª—å–Ω—ã–µ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ –¥–æ –Ω–∞—á–∞–ª–∞ –æ–±—É—á–µ–Ω–∏—è.
    """
    
    @classmethod
    def INPUT_TYPES(s):
        return {
            "required": {
                "model_type": (["flux2_klein_9b", "flux2_dev"], {"default": "flux2_klein_9b"}),
                "network_dim": ("INT", {"default": 16, "min": 1, "max": 256}),
                "batch_size": ("INT", {"default": 1, "min": 1, "max": 8}),
                "resolution": ("INT", {"default": 1024, "min": 512, "max": 2048, "step": 64}),
            },
            "optional": {
                "low_vram_config": ("FLUX2_LOW_VRAM_CONFIG",),
            }
        }

    RETURN_TYPES = ("STRING",)
    RETURN_NAMES = ("memory_report",)
    FUNCTION = "estimate"
    CATEGORY = "FluxTrainer/Flux2"
    OUTPUT_NODE = True

    def estimate(self, model_type, network_dim, batch_size, resolution, low_vram_config=None):
        # LAZY IMPORT - –∑–∞–≥—Ä—É–∑–∫–∞ —Ç–æ–ª—å–∫–æ –ø—Ä–∏ Queue Prompt
        modules = _lazy_import_training()
        
        # –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –º–æ–¥–µ–ª–µ–π
        model_params = {
            "flux2_klein_9b": 9.0,
            "flux2_dev": 32.0
        }
        
        params_b = model_params.get(model_type, 9.0)
        
        # –ï—Å–ª–∏ low_vram_config –Ω–µ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω, —Å–æ–∑–¥–∞—ë–º —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–π
        if low_vram_config is None:
            from dataclasses import dataclass
            from enum import Enum
            
            class VRAMStrategy(Enum):
                AGGRESSIVE = "aggressive_offload"
            
            @dataclass
            class DefaultConfig:
                strategy: VRAMStrategy = VRAMStrategy.AGGRESSIVE
                blocks_to_swap: int = 18
                
                def estimate_memory_usage(self, params_b: float):
                    return {
                        'base_model_vram': params_b * 0.3,
                        'lora_weights_vram': 0.1,
                        'activations_peak': 2.0,
                        'optimizer_vram': 0.5,
                        'optimizer_ram': 4.0
                    }
            
            low_vram_config = DefaultConfig()
        
        mem = low_vram_config.estimate_memory_usage(params_b)
        total_vram = sum(v for k, v in mem.items() if 'vram' in k)
        
        # –°—Ç–∞—Ç—É—Å
        if total_vram <= 8:
            status = "[OK] Should fit in 8GB VRAM"
        elif total_vram <= 12:
            status = "[WARN] May need 12GB VRAM"
        else:
            status = "[ERROR] Requires more than 12GB VRAM"
        
        report = f"""
‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó
‚ïë           FLUX.2 MEMORY ESTIMATION REPORT                    ‚ïë
‚ï†‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï£
‚ïë Model: {model_type:<54} ‚ïë
‚ïë Parameters: {params_b:.1f}B                                              ‚ïë
‚ïë Resolution: {resolution}x{resolution:<47} ‚ïë
‚ïë Batch Size: {batch_size:<52} ‚ïë
‚ïë LoRA Dim: {network_dim:<54} ‚ïë
‚ï†‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï£
‚ïë VRAM USAGE ESTIMATE:                                         ‚ïë
‚ïë   Base Model (FP8):     {mem['base_model_vram']:.1f} GB                              ‚ïë
‚ïë   LoRA Weights:         {mem['lora_weights_vram']:.2f} GB                              ‚ïë
‚ïë   Activations Peak:     {mem['activations_peak']:.1f} GB                              ‚ïë
‚ïë   Optimizer (GPU):      {mem['optimizer_vram']:.1f} GB                              ‚ïë
‚ïë   ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ    ‚ïë
‚ïë   TOTAL VRAM:           ~{total_vram:.1f} GB                             ‚ïë
‚ï†‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï£
‚ïë RAM USAGE ESTIMATE:                                          ‚ïë
‚ïë   Optimizer (CPU):      {mem['optimizer_ram']:.1f} GB                              ‚ïë
‚ïë   Cached TE Outputs:    ~2.0 GB                              ‚ïë
‚ï†‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï£
‚ïë STRATEGY: {low_vram_config.strategy.value:<52} ‚ïë
‚ïë Blocks to Swap: {low_vram_config.blocks_to_swap:<47} ‚ïë
‚ï†‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï£
‚ïë {status:<60} ‚ïë
‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù
"""
        
        return (report,)


# =============================================================================
# NODE MAPPINGS
# =============================================================================
# –ö–õ–Æ–ß–ï–í–û–ï –ò–ó–ú–ï–ù–ï–ù–ò–ï: –ù–æ–¥—ã –í–°–ï–ì–î–ê —Ä–µ–≥–∏—Å—Ç—Ä–∏—Ä—É—é—Ç—Å—è!
# –û—à–∏–±–∫–∏ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π –ø–æ—è–≤—è—Ç—Å—è —Ç–æ–ª—å–∫–æ –ø—Ä–∏ Queue Prompt, –∞ –Ω–µ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ.
# –≠—Ç–æ –ø–æ–∑–≤–æ–ª—è–µ—Ç ComfyUI –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ –æ—Ç–æ–±—Ä–∞–∂–∞—Ç—å –≤—Å–µ –Ω–æ–¥—ã –≤ UI.

NODE_CLASS_MAPPINGS = {
    "Flux2TrainModelSelect": Flux2TrainModelSelect,
    "Flux2TrainModelPaths": Flux2TrainModelPaths,
    "Flux2LowVRAMConfig": Flux2LowVRAMConfig,
    "Flux2InitTraining": Flux2InitTraining,
    "Flux2TrainLoop": Flux2TrainLoop,
    "Flux2TrainAndValidateLoop": Flux2TrainAndValidateLoop,
    "Flux2TrainSave": Flux2TrainSave,
    "Flux2TrainEnd": Flux2TrainEnd,
    "Flux2TrainAdvancedSettings": Flux2TrainAdvancedSettings,
    "Flux2MemoryEstimator": Flux2MemoryEstimator,
}

NODE_DISPLAY_NAME_MAPPINGS = {
    "Flux2TrainModelSelect": "üî∑ Flux.2 Model Select",
    "Flux2TrainModelPaths": "üìÅ Flux.2 Model Paths",
    "Flux2LowVRAMConfig": "üíæ Flux.2 Low VRAM Config",
    "Flux2InitTraining": "üöÄ Flux.2 Init Training",
    "Flux2TrainLoop": "üîÑ Flux.2 Train Loop",
    "Flux2TrainAndValidateLoop": "üîÑ‚úÖ Flux.2 Train & Validate",
    "Flux2TrainSave": "üíæ Flux.2 Save LoRA",
    "Flux2TrainEnd": "üèÅ Flux.2 End Training",
    "Flux2TrainAdvancedSettings": "‚öôÔ∏è Flux.2 Advanced Settings",
    "Flux2MemoryEstimator": "üìä Flux.2 Memory Estimator",
}

# Log registration
logger.info(f"[ComfyUI-FluxTrainer-Pro] Registered {len(NODE_CLASS_MAPPINGS)} Flux.2 nodes (lazy imports enabled)")
